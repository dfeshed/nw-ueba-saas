# Job
job.factory.class=fortscale.streaming.GracefulShutdownLocalJobFactory
job.name=service-account-tagging

# Task
task.class=fortscale.streaming.task.AccountTaggingTask

#will be change at the fuature the new kafka topic (with align headers)
task.inputs=kafka.fortscale-4769-structured

# export the state to mongodb every 15 minutes (15*60*1000=900000ms)
task.window.ms=900000


# Fortscale specific task config parameters
fortscale.context=classpath*:META-INF/spring/streaming-TaggingTask-context.xml
fortscale.username.field=normalized_username
fortscale.timestamp.field=date_time_unix
fortscale.sourceHostName.field=machine_name
fortscale.sourceComputerType.field=src_class
fortscale.destHostName.field=service_name
fortscale.destComputerType.field=dst_class

fortscale.isServiceAccount.field=isUserServiceAccount
fortscale.failureCode.field=failure_code
fortscale.isSensetiveMachine.field=is_sensitive_machine
fortscale.daysBack=30
fortscale.failureCodes = 0x0,


#store
fortscale.store.name=service-account-tagging-stats


# Serializers
serializers.registry.json.class=org.apache.samza.serializers.JsonSerdeFactory
serializers.registry.string.class=org.apache.samza.serializers.StringSerdeFactory
serializers.registry.integer.class=org.apache.samza.serializers.IntegerSerdeFactory
serializers.registry.jsonmodel.class=fortscale.streaming.serialization.AccountStateSerdeFactory
serializers.registry.metrics.class=org.apache.samza.serializers.MetricsSnapshotSerdeFactory


# Metric report every 60 seconds to a kafka topic called metrics and as a monitor report
metrics.reporter.snapshot.class=org.apache.samza.metrics.reporter.MetricsSnapshotReporterFactory
metrics.reporter.snapshot.stream=kafka.metrics
metrics.reporter.monitor.class=fortscale.streaming.metrics.MongoMetricsSnapshotReporterFactory
systems.kafka.streams.metrics.samza.msg.serde=metrics
metrics.reporters=snapshot,monitor


# Systems
systems.kafka.samza.factory=org.apache.samza.system.kafka.KafkaSystemFactory
systems.kafka.samza.msg.serde=string
systems.kafka.samza.offset.default=oldest
systems.kafka.consumer.zookeeper.connect=localhost:2181
systems.kafka.consumer.auto.offset.reset=smallest
systems.kafka.producer.metadata.broker.list=localhost:9092
systems.kafka.producer.producer.type=sync
systems.kafka.producer.retry.backoff.ms = 10000
systems.kafka.producer.acks = 1
systems.kafka.producer.reconnect.backoff.ms = 10000
# Normally, we'd set this much higher, but we want things to look snappy in the demo.
systems.kafka.producer.batch.num.messages=1


# Declare that we want our job's checkpoints to be enabled and written to Kafka
task.checkpoint.factory=org.apache.samza.checkpoint.kafka.KafkaCheckpointManagerFactory
task.checkpoint.replication.factor=1
task.checkpoint.system=kafka




#Key-value storage
stores.service-account-tagging-stats.factory=org.apache.samza.storage.kv.KeyValueStorageEngineFactory
stores.service-account-tagging-stats.changelog=kafka.service-account-tagging-stats-changelog
stores.service-account-tagging-stats.key.serde=string
stores.service-account-tagging-stats.msg.serde=jsonmodel



# This property is set to the number of key/value pairs that should be kept in this in-memory buffer, per task instance. The number cannot be greater than stores.*.object.cache.size.
stores.service-account-tagging-stats.write.batch.size=100
# This property determines the number of objects to keep in Samza's cache, per task instance. This same cache is also used for write buffering (see stores.*.write.batch.size). A value of 0 disables all caching and batching.
stores.service-account-tagging-stats.object.cache.size=2500
# The size of LevelDB's block cache in bytes, per container. Note that this is an off-heap memory allocation, so the container's total memory use is the maximum JVM heap size plus the size of this cache.
stores.service-account-tagging-stats.container.cache.size.bytes=5000000
# The amount of memory (in bytes) that LevelDB uses for buffering writes before they are written to disk,
stores.service-account-tagging-stats.container.write.buffer.size.bytes=100000

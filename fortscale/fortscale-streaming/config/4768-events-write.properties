# Job
job.factory.class=fortscale.streaming.GracefulShutdownLocalJobFactory
job.name=4768-events-hdfs-write

# Task
task.class=fortscale.streaming.task.HDFSWriterStreamTask
task.inputs=kafka.fortscale-4768-enriched-events
# flush the hdfs writers every 30 minutes (30*60*1000=300000ms)
task.window.ms=1800000
#task.opts=-agentlib:jdwp=transport=dt_socket,address=localhost:9009,server=y,suspend=y

# Fortscale specific task config parameters
fortscale.context=classpath*:META-INF/spring/streaming-context.xml
fortscale.timestamp.field=date_time_unix
fortscale.username.field=normalized_username
fortscale.discriminator.fields=recordNumber
fortscale.fields=impala.data.security.events.login.table.fields
fortscale.separator=,
fortscale.hdfs.root=/user/cloudera/data/login
fortscale.table.name=logindata
fortscale.file.name=datalogindata.csv
fortscale.partition.strategy=impala.data.security.events.login.table.partition.type
fortscale.split.strategy=fortscale.utils.hdfs.split.DailyFileSplitStrategy
# buffer no more than 1000 events before flushing to hdfs
fortscale.events.flush.threshold=1000


# Serializers
serializers.registry.json.class=org.apache.samza.serializers.JsonSerdeFactory
serializers.registry.string.class=org.apache.samza.serializers.StringSerdeFactory
serializers.registry.integer.class=org.apache.samza.serializers.IntegerSerdeFactory
serializers.registry.jsonmodel.class=fortscale.streaming.serialization.PrevalanceModelSerdeFactory
serializers.registry.long.class=fortscale.streaming.serialization.LongSerdeFactory
serializers.registry.metrics.class=org.apache.samza.serializers.MetricsSnapshotSerdeFactory
serializers.registry.timebarrier.class=fortscale.streaming.serialization.UserTimeBarrierModelSerdeFactory


# Metric report every 60 seconds to a kafka topic called metrics and as a monitor report
metrics.reporter.snapshot.class=org.apache.samza.metrics.reporter.MetricsSnapshotReporterFactory
metrics.reporter.snapshot.stream=kafka.metrics
metrics.reporter.monitor.class=fortscale.streaming.metrics.MongoMetricsSnapshotReporterFactory
systems.kafka.streams.metrics.samza.msg.serde=metrics
metrics.reporters=snapshot,monitor


# Systems
systems.kafka.samza.factory=org.apache.samza.system.kafka.KafkaSystemFactory
systems.kafka.samza.msg.serde=string
systems.kafka.samza.offset.default=oldest
systems.kafka.consumer.zookeeper.connect=localhost:2181
systems.kafka.consumer.auto.offset.reset=smallest
systems.kafka.producer.metadata.broker.list=localhost:9092
systems.kafka.producer.producer.type=async
systems.kafka.producer.retry.backoff.ms = 10000
systems.kafka.producer.acks = 1
systems.kafka.producer.reconnect.backoff.ms = 10000
systems.kafka.producer.queue.buffering.max.ms=2000
systems.kafka.producer.batch.num.messages=100

# Declare that we want our job's checkpoints to be enabled and written to Kafka
task.checkpoint.factory=org.apache.samza.checkpoint.kafka.KafkaCheckpointManagerFactory
task.checkpoint.replication.factor=1
task.checkpoint.system=kafka


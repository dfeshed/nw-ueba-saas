# Job
job.factory.class=fortscale.streaming.GracefulShutdownLocalJobFactory
job.name=hdfs-events-writer-task

# Task
task.class=fortscale.streaming.task.HDFSWriterStreamTask
# input topics
task.inputs=kafka.fortscale-4768-enriched-events,kafka.fortscale-4769-computer-tagged-clustered,kafka.fortscale-4769-event-score,kafka.fortscale-ssh-computer-tagged-clustered,kafka.fortscale-ssh-event-score,kafka.fortscale-vpn-geolocated-session-updated,kafka.fortscale-vpn-event-score,kafka.fortscale-vpnsession-event-score,kafka.fortscale-generic-data-access-normalized-tagged-event,kafka.fortscale-generic-data-access-computer-tagged-clustered,kafka.fortscale-generic-data-access-ip-geolocated,kafka.fortscale-generic-data-access-event-score
# flush the hdfs writers every 5 minutes (10*60*1000=300,000ms)
task.window.ms=300000


# Fortscale specific task config parameters
fortscale.context=classpath*:META-INF/spring/streaming-HdfsWriterTask-context.xml
fortscale.monitoring.enable=true



# 4768 enriched
fortscale.events.entry.name.4768_HDFSWriterStreamTask=4768_HDFSWriterStreamTask
fortscale.events.entry.4768_HDFSWriterStreamTask.data.source=kerberos_tgt
fortscale.events.entry.4768_HDFSWriterStreamTask.last.state=ComputerTaggingClusteringTask
fortscale.events.entry.4768_HDFSWriterStreamTask.timestamp.field=${impala.rawdata.security.events.login.table.field.date_time_unix}
fortscale.events.entry.4768_HDFSWriterStreamTask.username.field=${impala.data.security.events.login.table.field.normalized_username}
fortscale.events.entry.4768_HDFSWriterStreamTask.discriminator.fields=${impala.enricheddata.security.events.table.field.record_number}
fortscale.events.entry.4768_HDFSWriterStreamTask.fields=${impala.enricheddata.kerberos_tgt.table.fields}
fortscale.events.entry.4768_HDFSWriterStreamTask.separator=${impala.enricheddata.kerberos_tgt.table.delimiter}
fortscale.events.entry.4768_HDFSWriterStreamTask.hdfs.root=${hdfs.user.enricheddata.kerberos_tgt.path}
fortscale.events.entry.4768_HDFSWriterStreamTask.table.name=${impala.enricheddata.kerberos_tgt.table.name}
fortscale.events.entry.4768_HDFSWriterStreamTask.file.name=${hdfs.enricheddata.kerberos_tgt.file.name}
fortscale.events.entry.4768_HDFSWriterStreamTask.partition.strategy=${impala.enricheddata.kerberos_tgt.table.partition.type}
fortscale.events.entry.4768_HDFSWriterStreamTask.split.strategy=fortscale.utils.hdfs.split.DailyFileSplitStrategy
# Buffer no more than 10000 events before flushing to HDFS
fortscale.events.entry.4768_HDFSWriterStreamTask.events.flush.threshold=10000

# 4769 enriched
fortscale.events.entry.name.4769_enriched_HDFSWriterStreamTask=4769_enriched_HDFSWriterStreamTask
fortscale.events.entry.4769_enriched_HDFSWriterStreamTask.data.source=kerberos_logins
fortscale.events.entry.4769_enriched_HDFSWriterStreamTask.last.state=ComputerTaggingClusteringTask
fortscale.events.entry.4769_enriched_HDFSWriterStreamTask.output.topics=fortscale-4769-enriched-after-write
fortscale.events.entry.4769_enriched_HDFSWriterStreamTask.bdp.output.topics=
fortscale.events.entry.4769_enriched_HDFSWriterStreamTask.timestamp.field=${impala.enricheddata.security.events.table.field.date_time_unix}
fortscale.events.entry.4769_enriched_HDFSWriterStreamTask.username.field=${impala.enricheddata.security.events.table.field.normalized_username}
fortscale.events.entry.4769_enriched_HDFSWriterStreamTask.discriminator.fields=${impala.enricheddata.security.events.table.field.record_number}
fortscale.events.entry.4769_enriched_HDFSWriterStreamTask.fields=${impala.enricheddata.kerberos_logins.table.fields}
fortscale.events.entry.4769_enriched_HDFSWriterStreamTask.separator=${impala.enricheddata.kerberos_logins.table.delimiter}
fortscale.events.entry.4769_enriched_HDFSWriterStreamTask.hdfs.root=${hdfs.user.enricheddata.kerberos_logins.path}
fortscale.events.entry.4769_enriched_HDFSWriterStreamTask.table.name=${impala.enricheddata.kerberos_logins.table.name}
fortscale.events.entry.4769_enriched_HDFSWriterStreamTask.file.name=${hdfs.enricheddata.kerberos_logins.file.name}
fortscale.events.entry.4769_enriched_HDFSWriterStreamTask.partition.strategy=${impala.enricheddata.kerberos_logins.table.partition.type}
fortscale.events.entry.4769_enriched_HDFSWriterStreamTask.split.strategy=fortscale.utils.hdfs.split.DailyFileSplitStrategy
# Buffer no more than 10000 events before flushing to HDFS
fortscale.events.entry.4769_enriched_HDFSWriterStreamTask.events.flush.threshold=10000

# 4769 scored
fortscale.events.entry.name.4769_scored_HDFSWriterStreamTask=4769_scored_HDFSWriterStreamTask
fortscale.events.entry.4769_scored_HDFSWriterStreamTask.data.source=kerberos_logins
fortscale.events.entry.4769_scored_HDFSWriterStreamTask.last.state=MultipleEventsPrevalenceModelStreamTask
fortscale.events.entry.4769_scored_HDFSWriterStreamTask.output.topics=fortscale-4769-event-score-after-write,fortscale-4769-event-score-from-hdfs
fortscale.events.entry.4769_scored_HDFSWriterStreamTask.bdp.output.topics=fortscale-4769-event-score-after-write
fortscale.events.entry.4769_scored_HDFSWriterStreamTask.timestamp.field=${impala.score.ldapauth.table.fields.date_time_unix}
fortscale.events.entry.4769_scored_HDFSWriterStreamTask.username.field=${impala.score.ldapauth.table.fields.normalized_username}
fortscale.events.entry.4769_scored_HDFSWriterStreamTask.discriminator.fields=${impala.enricheddata.security.events.table.field.record_number}
fortscale.events.entry.4769_scored_HDFSWriterStreamTask.fields=${impala.score.kerberos_logins.table.fields}
fortscale.events.entry.4769_scored_HDFSWriterStreamTask.separator=${impala.score.kerberos_logins.table.delimiter}
fortscale.events.entry.4769_scored_HDFSWriterStreamTask.hdfs.root=${hdfs.user.processeddata.kerberos_logins.path}
fortscale.events.entry.4769_scored_HDFSWriterStreamTask.table.name=${impala.score.kerberos_logins.table.name}
fortscale.events.entry.4769_scored_HDFSWriterStreamTask.file.name=${hdfs.user.processeddata.kerberos_logins.file.name}
fortscale.events.entry.4769_scored_HDFSWriterStreamTask.partition.strategy=${impala.score.kerberos_logins.table.partition.type}
fortscale.events.entry.4769_scored_HDFSWriterStreamTask.split.strategy=fortscale.utils.hdfs.split.DailyFileSplitStrategy
# Buffer no more than 10000 events before flushing to HDFS
fortscale.events.entry.4769_scored_HDFSWriterStreamTask.events.flush.threshold=10000

# 4769 scored top
fortscale.events.entry.name.4769_scored_top_HDFSWriterStreamTask=4769_scored_top_HDFSWriterStreamTask
fortscale.events.entry.4769_scored_top_HDFSWriterStreamTask.data.source=kerberos_logins
fortscale.events.entry.4769_scored_top_HDFSWriterStreamTask.last.state=MultipleEventsPrevalenceModelStreamTask
fortscale.events.entry.4769_scored_top_HDFSWriterStreamTask.timestamp.field=${impala.score.ldapauth.table.fields.date_time_unix}
fortscale.events.entry.4769_scored_top_HDFSWriterStreamTask.username.field=${impala.score.ldapauth.table.fields.normalized_username}
fortscale.events.entry.4769_scored_top_HDFSWriterStreamTask.discriminator.fields=${impala.enricheddata.security.events.table.field.record_number}
fortscale.events.entry.4769_scored_top_HDFSWriterStreamTask.fields=${impala.score.kerberos_logins.table.fields}
fortscale.events.entry.4769_scored_top_HDFSWriterStreamTask.separator=${impala.score.kerberos_logins.top.table.delimiter}
fortscale.events.entry.4769_scored_top_HDFSWriterStreamTask.hdfs.root=${hdfs.user.processeddata.kerberos_logins.top.path}
fortscale.events.entry.4769_scored_top_HDFSWriterStreamTask.table.name=${impala.score.kerberos_logins.top.table.name}
fortscale.events.entry.4769_scored_top_HDFSWriterStreamTask.file.name=${hdfs.user.processeddata.kerberos_logins.file.name}
fortscale.events.entry.4769_scored_top_HDFSWriterStreamTask.partition.strategy=${impala.score.kerberos_logins.top.table.partition.type}
fortscale.events.entry.4769_scored_top_HDFSWriterStreamTask.split.strategy=fortscale.utils.hdfs.split.DailyFileSplitStrategy
# Buffer no more than 10000 events before flushing to HDFS
fortscale.events.entry.4769_scored_top_HDFSWriterStreamTask.events.flush.threshold=10000
#Filter configuration moved into streaming-HdfsWriterTask-context.xml
fortscale.events.entry.4769_scored_top_HDFSWriterStreamTask.filters=4769ScoredTopScoreFilter

# SSH enriched
fortscale.events.entry.name.ssh_enriched_HDFSWriterStreamTask=ssh_enriched_HDFSWriterStreamTask
fortscale.events.entry.ssh_enriched_HDFSWriterStreamTask.data.source=ssh
fortscale.events.entry.ssh_enriched_HDFSWriterStreamTask.last.state=ComputerTaggingClusteringTask
fortscale.events.entry.ssh_enriched_HDFSWriterStreamTask.output.topics=fortscale-ssh-enriched-after-write
fortscale.events.entry.ssh_enriched_HDFSWriterStreamTask.bdp.output.topics=
fortscale.events.entry.ssh_enriched_HDFSWriterStreamTask.timestamp.field=${impala.enricheddata.ssh.table.field.date_time_unix}
fortscale.events.entry.ssh_enriched_HDFSWriterStreamTask.username.field=${impala.enricheddata.ssh.table.field.normalized_username}
fortscale.events.entry.ssh_enriched_HDFSWriterStreamTask.discriminator.fields=${impala.enricheddata.ssh.table.field.target_machine}
fortscale.events.entry.ssh_enriched_HDFSWriterStreamTask.fields=${impala.enricheddata.ssh.table.fields}
fortscale.events.entry.ssh_enriched_HDFSWriterStreamTask.separator=${impala.enricheddata.ssh.table.delimiter}
fortscale.events.entry.ssh_enriched_HDFSWriterStreamTask.hdfs.root=${hdfs.user.enricheddata.ssh.path}
fortscale.events.entry.ssh_enriched_HDFSWriterStreamTask.table.name=${impala.enricheddata.ssh.table.name}
fortscale.events.entry.ssh_enriched_HDFSWriterStreamTask.file.name=${hdfs.enricheddata.ssh.file.name}
fortscale.events.entry.ssh_enriched_HDFSWriterStreamTask.partition.strategy=${impala.enricheddata.ssh.table.partition.type}
fortscale.events.entry.ssh_enriched_HDFSWriterStreamTask.split.strategy=fortscale.utils.hdfs.split.DailyFileSplitStrategy
# Buffer no more than 10000 events before flushing to HDFS
fortscale.events.entry.ssh_enriched_HDFSWriterStreamTask.events.flush.threshold=10000

# SSH scored
fortscale.events.entry.name.ssh_scored_HDFSWriterStreamTask=ssh_scored_HDFSWriterStreamTask
fortscale.events.entry.ssh_scored_HDFSWriterStreamTask.data.source=ssh
fortscale.events.entry.ssh_scored_HDFSWriterStreamTask.last.state=MultipleEventsPrevalenceModelStreamTask
fortscale.events.entry.ssh_scored_HDFSWriterStreamTask.output.topics=fortscale-ssh-event-score-after-write,fortscale-ssh-event-score-from-hdfs
fortscale.events.entry.ssh_scored_HDFSWriterStreamTask.bdp.output.topics=fortscale-ssh-event-score-after-write
fortscale.events.entry.ssh_scored_HDFSWriterStreamTask.timestamp.field=${impala.data.table.fields.epochtime}
fortscale.events.entry.ssh_scored_HDFSWriterStreamTask.username.field=${impala.data.table.fields.normalized_username}
fortscale.events.entry.ssh_scored_HDFSWriterStreamTask.discriminator.fields=${impala.score.ssh.table.field.target_machine}
fortscale.events.entry.ssh_scored_HDFSWriterStreamTask.fields=${impala.score.ssh.table.fields}
fortscale.events.entry.ssh_scored_HDFSWriterStreamTask.separator=${impala.score.ssh.table.delimiter}
fortscale.events.entry.ssh_scored_HDFSWriterStreamTask.hdfs.root=${hdfs.user.processeddata.ssh.path}
fortscale.events.entry.ssh_scored_HDFSWriterStreamTask.table.name=${impala.score.ssh.table.name}
fortscale.events.entry.ssh_scored_HDFSWriterStreamTask.file.name=${hdfs.user.processeddata.ssh.file.name}
fortscale.events.entry.ssh_scored_HDFSWriterStreamTask.partition.strategy=${impala.score.ssh.table.partition.type}
fortscale.events.entry.ssh_scored_HDFSWriterStreamTask.split.strategy=fortscale.utils.hdfs.split.DailyFileSplitStrategy
# Buffer no more than 10000 events before flushing to HDFS
fortscale.events.entry.ssh_scored_HDFSWriterStreamTask.events.flush.threshold=10000

# SSH scored top
fortscale.events.entry.name.ssh_scored_top_HDFSWriterStreamTask=ssh_scored_top_HDFSWriterStreamTask
fortscale.events.entry.ssh_scored_top_HDFSWriterStreamTask.data.source=ssh
fortscale.events.entry.ssh_scored_top_HDFSWriterStreamTask.last.state=MultipleEventsPrevalenceModelStreamTask
fortscale.events.entry.ssh_scored_top_HDFSWriterStreamTask.timestamp.field=${impala.data.table.fields.epochtime}
fortscale.events.entry.ssh_scored_top_HDFSWriterStreamTask.username.field=${impala.data.table.fields.normalized_username}
fortscale.events.entry.ssh_scored_top_HDFSWriterStreamTask.discriminator.fields=${impala.score.ssh.table.field.target_machine}
fortscale.events.entry.ssh_scored_top_HDFSWriterStreamTask.fields=${impala.score.ssh.table.fields}
fortscale.events.entry.ssh_scored_top_HDFSWriterStreamTask.separator=${impala.score.ssh.top.table.delimiter}
fortscale.events.entry.ssh_scored_top_HDFSWriterStreamTask.hdfs.root=${hdfs.user.processeddata.ssh.top.path}
fortscale.events.entry.ssh_scored_top_HDFSWriterStreamTask.table.name=${impala.score.ssh.top.table.name}
fortscale.events.entry.ssh_scored_top_HDFSWriterStreamTask.file.name=${hdfs.user.processeddata.ssh.file.name}
fortscale.events.entry.ssh_scored_top_HDFSWriterStreamTask.partition.strategy=${impala.score.ssh.top.table.partition.type}
fortscale.events.entry.ssh_scored_top_HDFSWriterStreamTask.split.strategy=fortscale.utils.hdfs.split.DailyFileSplitStrategy
# Buffer no more than 10000 events before flushing to HDFS
fortscale.events.entry.ssh_scored_top_HDFSWriterStreamTask.events.flush.threshold=10000
#Filter configuration moved into streaming-HdfsWriterTask-context.xml
fortscale.events.entry.ssh_scored_top_HDFSWriterStreamTask.filters=sshScoredTopScoreFilter

# VPN enriched
fortscale.events.entry.name.vpn_enriched_HDFSWriterStreamTask=vpn_enriched_HDFSWriterStreamTask
fortscale.events.entry.vpn_enriched_HDFSWriterStreamTask.data.source=vpn
fortscale.events.entry.vpn_enriched_HDFSWriterStreamTask.last.state=VpnEnrichTask
fortscale.events.entry.vpn_enriched_HDFSWriterStreamTask.output.topics=fortscale-vpn-enriched-after-write
fortscale.events.entry.vpn_enriched_HDFSWriterStreamTask.bdp.output.topics=
fortscale.events.entry.vpn_enriched_HDFSWriterStreamTask.timestamp.field=${impala.enricheddata.vpn.table.field.date_time_unix}
fortscale.events.entry.vpn_enriched_HDFSWriterStreamTask.username.field=${impala.enricheddata.vpn.table.field.normalized_username}
fortscale.events.entry.vpn_enriched_HDFSWriterStreamTask.discriminator.fields=${impala.enricheddata.vpn.table.field.local_ip}
fortscale.events.entry.vpn_enriched_HDFSWriterStreamTask.fields=${impala.enricheddata.vpn.table.fields}
fortscale.events.entry.vpn_enriched_HDFSWriterStreamTask.separator=${impala.enricheddata.vpn.table.delimiter}
fortscale.events.entry.vpn_enriched_HDFSWriterStreamTask.hdfs.root=${hdfs.user.enricheddata.vpn.path}
fortscale.events.entry.vpn_enriched_HDFSWriterStreamTask.table.name=${impala.enricheddata.vpn.table.name}
fortscale.events.entry.vpn_enriched_HDFSWriterStreamTask.file.name=${hdfs.enricheddata.vpn.file.name}
fortscale.events.entry.vpn_enriched_HDFSWriterStreamTask.partition.strategy=${impala.enricheddata.vpn.table.partition.type}
fortscale.events.entry.vpn_enriched_HDFSWriterStreamTask.split.strategy=fortscale.utils.hdfs.split.DailyFileSplitStrategy
# Buffer no more than 10000 events before flushing to HDFS
fortscale.events.entry.vpn_enriched_HDFSWriterStreamTask.events.flush.threshold=10000

# VPN scored
fortscale.events.entry.name.vpn_scored_HDFSWriterStreamTask=vpn_scored_HDFSWriterStreamTask
fortscale.events.entry.vpn_scored_HDFSWriterStreamTask.data.source=vpn
fortscale.events.entry.vpn_scored_HDFSWriterStreamTask.last.state=MultipleEventsPrevalenceModelStreamTask
fortscale.events.entry.vpn_scored_HDFSWriterStreamTask.output.topics=fortscale-vpn-event-score-after-write,fortscale-vpn-event-score-from-hdfs
fortscale.events.entry.vpn_scored_HDFSWriterStreamTask.bdp.output.topics=fortscale-vpn-event-score-after-write
fortscale.events.entry.vpn_scored_HDFSWriterStreamTask.timestamp.field=${impala.score.vpn.table.field.date_time_unix}
fortscale.events.entry.vpn_scored_HDFSWriterStreamTask.username.field=${impala.score.vpn.table.field.normalized_username}
fortscale.events.entry.vpn_scored_HDFSWriterStreamTask.discriminator.fields=${impala.score.vpn.table.field.local_ip}
fortscale.events.entry.vpn_scored_HDFSWriterStreamTask.fields=${impala.score.vpn.table.fields}
fortscale.events.entry.vpn_scored_HDFSWriterStreamTask.separator=${impala.score.vpn.top.table.delimiter}
fortscale.events.entry.vpn_scored_HDFSWriterStreamTask.hdfs.root=${hdfs.user.processeddata.vpn.path}
fortscale.events.entry.vpn_scored_HDFSWriterStreamTask.table.name=${impala.score.vpn.table.name}
fortscale.events.entry.vpn_scored_HDFSWriterStreamTask.file.name=${hdfs.user.processeddata.vpn.file.name}
fortscale.events.entry.vpn_scored_HDFSWriterStreamTask.partition.strategy=${impala.score.vpn.table.partition.type}
fortscale.events.entry.vpn_scored_HDFSWriterStreamTask.split.strategy=fortscale.utils.hdfs.split.DailyFileSplitStrategy
# Buffer no more than 10000 events before flushing to HDFS
fortscale.events.entry.vpn_scored_HDFSWriterStreamTask.events.flush.threshold=10000

# top VPN scored
fortscale.events.entry.name.vpn_top_scored_HDFSWriterStreamTask=vpn_top_scored_HDFSWriterStreamTask
fortscale.events.entry.vpn_top_scored_HDFSWriterStreamTask.data.source=vpn
fortscale.events.entry.vpn_top_scored_HDFSWriterStreamTask.last.state=MultipleEventsPrevalenceModelStreamTask
fortscale.events.entry.vpn_top_scored_HDFSWriterStreamTask.timestamp.field=${impala.score.vpn.table.field.date_time_unix}
fortscale.events.entry.vpn_top_scored_HDFSWriterStreamTask.username.field=${impala.score.vpn.table.field.normalized_username}
fortscale.events.entry.vpn_top_scored_HDFSWriterStreamTask.discriminator.fields=${impala.score.vpn.table.field.local_ip}
fortscale.events.entry.vpn_top_scored_HDFSWriterStreamTask.fields=${impala.score.vpn.table.fields}
fortscale.events.entry.vpn_top_scored_HDFSWriterStreamTask.separator=${impala.score.vpn.top.table.delimiter}
fortscale.events.entry.vpn_top_scored_HDFSWriterStreamTask.hdfs.root=${hdfs.user.processeddata.vpn.top.path}
fortscale.events.entry.vpn_top_scored_HDFSWriterStreamTask.table.name=${impala.score.vpn.top.table.name}
fortscale.events.entry.vpn_top_scored_HDFSWriterStreamTask.file.name=${hdfs.user.processeddata.vpn.file.name}
fortscale.events.entry.vpn_top_scored_HDFSWriterStreamTask.partition.strategy=${impala.score.vpn.top.table.partition.type}
fortscale.events.entry.vpn_top_scored_HDFSWriterStreamTask.split.strategy=fortscale.utils.hdfs.split.DailyFileSplitStrategy
# Buffer no more than 10000 events before flushing to HDFS
fortscale.events.entry.vpn_top_scored_HDFSWriterStreamTask.events.flush.threshold=10000
#Filter configuration moved into streaming-HdfsWriterTask-context.xml
fortscale.events.entry.vpn_top_scored_HDFSWriterStreamTask.filters=vpnScoredTopScoreFilter


# VPN scored session
fortscale.events.entry.name.vpn_session_scored_HDFSWriterStreamTask=vpn_session_scored_HDFSWriterStreamTask
fortscale.events.entry.vpn_session_scored_HDFSWriterStreamTask.data.source=vpn_session
fortscale.events.entry.vpn_session_scored_HDFSWriterStreamTask.last.state=MultipleEventsPrevalenceModelStreamTask
fortscale.events.entry.vpn_session_scored_HDFSWriterStreamTask.output.topics=fortscale-vpnsession-event-score-after-write,fortscale-vpnsession-event-score-from-hdfs
fortscale.events.entry.vpn_session_scored_HDFSWriterStreamTask.bdp.output.topics=fortscale-vpnsession-event-score-after-write
fortscale.events.entry.vpn_session_scored_HDFSWriterStreamTask.timestamp.field=${impala.data.vpn.table.field.epochtime}
fortscale.events.entry.vpn_session_scored_HDFSWriterStreamTask.username.field=${impala.data.table.fields.normalized_username}
fortscale.events.entry.vpn_session_scored_HDFSWriterStreamTask.discriminator.fields=${impala.score.vpn.session.table.field.local_ip}
fortscale.events.entry.vpn_session_scored_HDFSWriterStreamTask.fields=${impala.score.vpn_session.table.fields}
fortscale.events.entry.vpn_session_scored_HDFSWriterStreamTask.separator=${impala.score.vpn_session.table.delimiter}
fortscale.events.entry.vpn_session_scored_HDFSWriterStreamTask.hdfs.root=${hdfs.user.processeddata.vpn_session.path}
fortscale.events.entry.vpn_session_scored_HDFSWriterStreamTask.table.name=${impala.score.vpn_session.table.name}
fortscale.events.entry.vpn_session_scored_HDFSWriterStreamTask.file.name=${hdfs.user.processeddata.vpn_session.file.name}
fortscale.events.entry.vpn_session_scored_HDFSWriterStreamTask.partition.strategy=${impala.score.vpn_session.table.partition.type}
fortscale.events.entry.vpn_session_scored_HDFSWriterStreamTask.split.strategy=fortscale.utils.hdfs.split.DailyFileSplitStrategy
# Buffer no more than 10000 events before flushing to HDFS
fortscale.events.entry.vpn_session_scored_HDFSWriterStreamTask.events.flush.threshold=10000

# top VPN scored session
fortscale.events.entry.name.vpn_session_top_scored_HDFSWriterStreamTask=vpn_session_top_scored_HDFSWriterStreamTask
fortscale.events.entry.vpn_session_top_scored_HDFSWriterStreamTask.data.source=vpn_session
fortscale.events.entry.vpn_session_top_scored_HDFSWriterStreamTask.last.state=MultipleEventsPrevalenceModelStreamTask
fortscale.events.entry.vpn_session_top_scored_HDFSWriterStreamTask.input.topic=fortscale-vpnsession-event-score
fortscale.events.entry.vpn_session_top_scored_HDFSWriterStreamTask.timestamp.field=${impala.data.vpn.table.field.epochtime}
fortscale.events.entry.vpn_session_top_scored_HDFSWriterStreamTask.username.field=${impala.data.table.fields.normalized_username}
fortscale.events.entry.vpn_session_top_scored_HDFSWriterStreamTask.discriminator.fields=${impala.score.vpn.session.table.field.local_ip}
fortscale.events.entry.vpn_session_top_scored_HDFSWriterStreamTask.fields=${impala.score.vpn_session.table.fields}
fortscale.events.entry.vpn_session_top_scored_HDFSWriterStreamTask.separator=${impala.score.vpn_session.top.table.delimiter}
fortscale.events.entry.vpn_session_top_scored_HDFSWriterStreamTask.hdfs.root=${hdfs.user.processeddata.vpn_session.top.path}
fortscale.events.entry.vpn_session_top_scored_HDFSWriterStreamTask.table.name=${impala.score.vpn_session.top.table.name}
fortscale.events.entry.vpn_session_top_scored_HDFSWriterStreamTask.file.name=${hdfs.user.processeddata.vpn_session.file.name}
fortscale.events.entry.vpn_session_top_scored_HDFSWriterStreamTask.partition.strategy=${impala.score.vpn_session.top.table.partition.type}
fortscale.events.entry.vpn_session_top_scored_HDFSWriterStreamTask.split.strategy=fortscale.utils.hdfs.split.DailyFileSplitStrategy
# Buffer no more than 10000 events before flushing to HDFS
fortscale.events.entry.vpn_session_top_scored_HDFSWriterStreamTask.events.flush.threshold=10000
#Filter configuration moved into streaming-HdfsWriterTask-context.xml
fortscale.events.entry.vpn_session_top_scored_HDFSWriterStreamTask.filters=vpnSessionScoredTopScoreFilter


# Serializers
serializers.registry.json.class=org.apache.samza.serializers.JsonSerdeFactory
serializers.registry.string.class=org.apache.samza.serializers.StringSerdeFactory
serializers.registry.integer.class=org.apache.samza.serializers.IntegerSerdeFactory
serializers.registry.long.class=fortscale.streaming.serialization.LongSerdeFactory
serializers.registry.metrics.class=org.apache.samza.serializers.MetricsSnapshotSerdeFactory
serializers.registry.timebarrier.class=fortscale.streaming.serialization.UserTimeBarrierModelSerdeFactory

# Metric report every 60 seconds to a kafka topic called metrics and as a monitor report
metrics.reporter.snapshot.class=org.apache.samza.metrics.reporter.MetricsSnapshotReporterFactory
metrics.reporter.snapshot.stream=kafka.metrics
systems.kafka.streams.metrics.samza.msg.serde=metrics
metrics.reporters=snapshot

# Systems
systems.kafka.samza.factory=org.apache.samza.system.kafka.KafkaSystemFactory
systems.kafka.samza.msg.serde=string
systems.kafka.samza.offset.default=oldest
systems.kafka.consumer.zookeeper.connect=localhost:2181
systems.kafka.consumer.auto.offset.reset=smallest
systems.kafka.producer.bootstrap.servers=localhost:9092
systems.kafka.producer.retry.backoff.ms=10000
systems.kafka.producer.acks=1
#systems.kafka.producer.reconnect.backoff.ms=10000

# Declare that we want our job's checkpoints to be enabled and written to Kafka
task.checkpoint.factory=org.apache.samza.checkpoint.kafka.KafkaCheckpointManagerFactory
task.checkpoint.replication.factor=1
task.checkpoint.system=kafka

# Key-value storage
####################

# 4768 enriched
stores.hdfs-write-logindata.factory=org.apache.samza.storage.kv.RocksDbKeyValueStorageEngineFactory
stores.hdfs-write-logindata.changelog=kafka.hdfs-write-logindata-changelog
stores.hdfs-write-logindata.changelog.replication.factor=1
stores.hdfs-write-logindata.key.serde=string
stores.hdfs-write-logindata.msg.serde=timebarrier
# This property is set to the number of key/value pairs that should be kept in this in-memory buffer, per task instance. The number cannot be greater than stores.*.object.cache.size.
stores.hdfs-write-logindata.write.batch.size=25
# This property determines the number of objects to keep in Samza's cache, per task instance. This same cache is also used for write buffering (see stores.*.write.batch.size). A value of 0 disables all caching and batching.
stores.hdfs-write-logindata.object.cache.size=100
# The size of LevelDB's block cache in bytes, per container. Note that this is an off-heap memory allocation, so the container's total memory use is the maximum JVM heap size plus the size of this cache.
stores.hdfs-write-logindata.container.cache.size.bytes=2000
# The amount of memory (in bytes) that LevelDB uses for buffering writes before they are written to disk,
stores.hdfs-write-logindata.container.write.buffer.size.bytes=1000

# 4769 enriched
stores.hdfs-write-4769enriched.factory=org.apache.samza.storage.kv.RocksDbKeyValueStorageEngineFactory
stores.hdfs-write-4769enriched.changelog=kafka.hdfs-write-4769enriched-changelog
stores.hdfs-write-4769enriched.changelog.replication.factor=1
stores.hdfs-write-4769enriched.key.serde=string
stores.hdfs-write-4769enriched.msg.serde=timebarrier
# This property is set to the number of key/value pairs that should be kept in this in-memory buffer, per task instance. The number cannot be greater than stores.*.object.cache.size.
stores.hdfs-write-4769enriched.write.batch.size=25
# This property determines the number of objects to keep in Samza's cache, per task instance. This same cache is also used for write buffering (see stores.*.write.batch.size). A value of 0 disables all caching and batching.
stores.hdfs-write-4769enriched.object.cache.size=100
# The size of LevelDB's block cache in bytes, per container. Note that this is an off-heap memory allocation, so the container's total memory use is the maximum JVM heap size plus the size of this cache.
stores.hdfs-write-4769enriched.container.cache.size.bytes=2000
# The amount of memory (in bytes) that LevelDB uses for buffering writes before they are written to disk.
stores.hdfs-write-4769enriched.container.write.buffer.size.bytes=1000

# 4769 scored
stores.hdfs-write-authenticationscores.factory=org.apache.samza.storage.kv.RocksDbKeyValueStorageEngineFactory
stores.hdfs-write-authenticationscores.changelog=kafka.hdfs-write-authenticationscores-changelog
stores.hdfs-write-authenticationscores.changelog.replication.factor=1
stores.hdfs-write-authenticationscores.key.serde=string
stores.hdfs-write-authenticationscores.msg.serde=timebarrier
# This property is set to the number of key/value pairs that should be kept in this in-memory buffer, per task instance. The number cannot be greater than stores.*.object.cache.size.
stores.hdfs-write-authenticationscores.write.batch.size=25
# This property determines the number of objects to keep in Samza's cache, per task instance. This same cache is also used for write buffering (see stores.*.write.batch.size). A value of 0 disables all caching and batching.
stores.hdfs-write-authenticationscores.object.cache.size=100
# The size of LevelDB's block cache in bytes, per container. Note that this is an off-heap memory allocation, so the container's total memory use is the maximum JVM heap size plus the size of this cache.
stores.hdfs-write-authenticationscores.container.cache.size.bytes=2000
# The amount of memory (in bytes) that LevelDB uses for buffering writes before they are written to disk,
stores.hdfs-write-authenticationscores.container.write.buffer.size.bytes=1000

# 4769 scored top
stores.hdfs-write-authenticationscores_top.factory=org.apache.samza.storage.kv.RocksDbKeyValueStorageEngineFactory
stores.hdfs-write-authenticationscores_top.changelog=kafka.hdfs-write-authenticationscores_top-changelog
stores.hdfs-write-authenticationscores_top.changelog.replication.factor=1
stores.hdfs-write-authenticationscores_top.key.serde=string
stores.hdfs-write-authenticationscores_top.msg.serde=timebarrier
# This property is set to the number of key/value pairs that should be kept in this in-memory buffer, per task instance. The number cannot be greater than stores.*.object.cache.size.
stores.hdfs-write-authenticationscores_top.write.batch.size=25
# This property determines the number of objects to keep in Samza's cache, per task instance. This same cache is also used for write buffering (see stores.*.write.batch.size). A value of 0 disables all caching and batching.
stores.hdfs-write-authenticationscores_top.object.cache.size=100
# The size of LevelDB's block cache in bytes, per container. Note that this is an off-heap memory allocation, so the container's total memory use is the maximum JVM heap size plus the size of this cache.
stores.hdfs-write-authenticationscores_top.container.cache.size.bytes=2000
# The amount of memory (in bytes) that LevelDB uses for buffering writes before they are written to disk,
stores.hdfs-write-authenticationscores_top.container.write.buffer.size.bytes=1000

# SSH enriched
stores.hdfs-write-sshenriched.factory=org.apache.samza.storage.kv.RocksDbKeyValueStorageEngineFactory
stores.hdfs-write-sshenriched.changelog=kafka.hdfs-write-sshenriched-changelog
stores.hdfs-write-sshenriched.changelog.replication.factor=1
stores.hdfs-write-sshenriched.key.serde=string
stores.hdfs-write-sshenriched.msg.serde=timebarrier
# This property is set to the number of key/value pairs that should be kept in this in-memory buffer, per task instance. The number cannot be greater than stores.*.object.cache.size.
stores.hdfs-write-sshenriched.write.batch.size=25
# This property determines the number of objects to keep in Samza's cache, per task instance. This same cache is also used for write buffering (see stores.*.write.batch.size). A value of 0 disables all caching and batching.
stores.hdfs-write-sshenriched.object.cache.size=100
# The size of LevelDB's block cache in bytes, per container. Note that this is an off-heap memory allocation, so the container's total memory use is the maximum JVM heap size plus the size of this cache.
stores.hdfs-write-sshenriched.container.cache.size.bytes=2000
# The amount of memory (in bytes) that LevelDB uses for buffering writes before they are written to disk.
stores.hdfs-write-sshenriched.container.write.buffer.size.bytes=1000

# SSH scored
stores.hdfs-write-sshscores.factory=org.apache.samza.storage.kv.RocksDbKeyValueStorageEngineFactory
stores.hdfs-write-sshscores.changelog=kafka.hdfs-write-sshscores-changelog
stores.hdfs-write-sshscores.changelog.replication.factor=1
stores.hdfs-write-sshscores.key.serde=string
stores.hdfs-write-sshscores.msg.serde=timebarrier
# This property is set to the number of key/value pairs that should be kept in this in-memory buffer, per task instance. The number cannot be greater than stores.*.object.cache.size.
stores.hdfs-write-sshscores.write.batch.size=25
# This property determines the number of objects to keep in Samza's cache, per task instance. This same cache is also used for write buffering (see stores.*.write.batch.size). A value of 0 disables all caching and batching.
stores.hdfs-write-sshscores.object.cache.size=100
# The size of LevelDB's block cache in bytes, per container. Note that this is an off-heap memory allocation, so the container's total memory use is the maximum JVM heap size plus the size of this cache.
stores.hdfs-write-sshscores.container.cache.size.bytes=2000
# The amount of memory (in bytes) that LevelDB uses for buffering writes before they are written to disk,
stores.hdfs-write-sshscores.container.write.buffer.size.bytes=1000

# SSH scored top
stores.hdfs-write-sshscores_top.factory=org.apache.samza.storage.kv.RocksDbKeyValueStorageEngineFactory
stores.hdfs-write-sshscores_top.changelog=kafka.hdfs-write-sshscores_top-changelog
stores.hdfs-write-sshscores_top.changelog.replication.factor=1
stores.hdfs-write-sshscores_top.key.serde=string
stores.hdfs-write-sshscores_top.msg.serde=timebarrier
# This property is set to the number of key/value pairs that should be kept in this in-memory buffer, per task instance. The number cannot be greater than stores.*.object.cache.size.
stores.hdfs-write-sshscores_top.write.batch.size=25
# This property determines the number of objects to keep in Samza's cache, per task instance. This same cache is also used for write buffering (see stores.*.write.batch.size). A value of 0 disables all caching and batching.
stores.hdfs-write-sshscores_top.object.cache.size=100
# The size of LevelDB's block cache in bytes, per container. Note that this is an off-heap memory allocation, so the container's total memory use is the maximum JVM heap size plus the size of this cache.
stores.hdfs-write-sshscores_top.container.cache.size.bytes=2000
# The amount of memory (in bytes) that LevelDB uses for buffering writes before they are written to disk,
stores.hdfs-write-sshscores_top.container.write.buffer.size.bytes=1000

# VPN enriched
stores.hdfs-write-vpnenriched.factory=org.apache.samza.storage.kv.RocksDbKeyValueStorageEngineFactory
stores.hdfs-write-vpnenriched.changelog=kafka.hdfs-write-vpnenriched-changelog
stores.hdfs-write-vpnenriched.changelog.replication.factor=1
stores.hdfs-write-vpnenriched.key.serde=string
stores.hdfs-write-vpnenriched.msg.serde=timebarrier
# This property is set to the number of key/value pairs that should be kept in this in-memory buffer, per task instance. The number cannot be greater than stores.*.object.cache.size.
stores.hdfs-write-vpnenriched.write.batch.size=25
# This property determines the number of objects to keep in Samza's cache, per task instance. This same cache is also used for write buffering (see stores.*.write.batch.size). A value of 0 disables all caching and batching.
stores.hdfs-write-vpnenriched.object.cache.size=100
# The size of LevelDB's block cache in bytes, per container. Note that this is an off-heap memory allocation, so the container's total memory use is the maximum JVM heap size plus the size of this cache.
stores.hdfs-write-vpnenriched.container.cache.size.bytes=2000
# The amount of memory (in bytes) that LevelDB uses for buffering writes before they are written to disk.
stores.hdfs-write-vpnenriched.container.write.buffer.size.bytes=1000

# VPN scored
stores.hdfs-write-vpndatares.factory=org.apache.samza.storage.kv.RocksDbKeyValueStorageEngineFactory
stores.hdfs-write-vpndatares.changelog=kafka.hdfs-write-vpndatares-changelog
stores.hdfs-write-vpndatares.changelog.replication.factor=1
stores.hdfs-write-vpndatares.key.serde=string
stores.hdfs-write-vpndatares.msg.serde=timebarrier
# This property is set to the number of key/value pairs that should be kept in this in-memory buffer, per task instance. The number cannot be greater than stores.*.object.cache.size.
stores.hdfs-write-vpndatares.write.batch.size=25
# This property determines the number of objects to keep in Samza's cache, per task instance. This same cache is also used for write buffering (see stores.*.write.batch.size). A value of 0 disables all caching and batching.
stores.hdfs-write-vpndatares.object.cache.size=100
# The size of LevelDB's block cache in bytes, per container. Note that this is an off-heap memory allocation, so the container's total memory use is the maximum JVM heap size plus the size of this cache.
stores.hdfs-write-vpndatares.container.cache.size.bytes=2000
# The amount of memory (in bytes) that LevelDB uses for buffering writes before they are written to disk,
stores.hdfs-write-vpndatares.container.write.buffer.size.bytes=1000

# VPN scored top
stores.hdfs-write-vpndatares_top.factory=org.apache.samza.storage.kv.RocksDbKeyValueStorageEngineFactory
stores.hdfs-write-vpndatares_top.changelog=kafka.hdfs-write-vpndatares_top-changelog
stores.hdfs-write-vpndatares_top.changelog.replication.factor=1
stores.hdfs-write-vpndatares_top.key.serde=string
stores.hdfs-write-vpndatares_top.msg.serde=timebarrier
# This property is set to the number of key/value pairs that should be kept in this in-memory buffer, per task instance. The number cannot be greater than stores.*.object.cache.size.
stores.hdfs-write-vpndatares_top.write.batch.size=25
# This property determines the number of objects to keep in Samza's cache, per task instance. This same cache is also used for write buffering (see stores.*.write.batch.size). A value of 0 disables all caching and batching.
stores.hdfs-write-vpndatares_top.object.cache.size=100
# The size of LevelDB's block cache in bytes, per container. Note that this is an off-heap memory allocation, so the container's total memory use is the maximum JVM heap size plus the size of this cache.
stores.hdfs-write-vpndatares_top.container.cache.size.bytes=2000
# The amount of memory (in bytes) that LevelDB uses for buffering writes before they are written to disk,
stores.hdfs-write-vpndatares_top.container.write.buffer.size.bytes=1000

# VPN scored session
stores.hdfs-write-vpnsessiondatares.factory=org.apache.samza.storage.kv.RocksDbKeyValueStorageEngineFactory
stores.hdfs-write-vpnsessiondatares.changelog=kafka.hdfs-write-vpnsessiondatares-changelog
stores.hdfs-write-vpnsessiondatares.changelog.replication.factor=1
stores.hdfs-write-vpnsessiondatares.key.serde=string
stores.hdfs-write-vpnsessiondatares.msg.serde=timebarrier
# This property is set to the number of key/value pairs that should be kept in this in-memory buffer, per task instance. The number cannot be greater than stores.*.object.cache.size.
stores.hdfs-write-vpnsessiondatares.write.batch.size=25
# This property determines the number of objects to keep in Samza's cache, per task instance. This same cache is also used for write buffering (see stores.*.write.batch.size). A value of 0 disables all caching and batching.
stores.hdfs-write-vpnsessiondatares.object.cache.size=100
# The size of LevelDB's block cache in bytes, per container. Note that this is an off-heap memory allocation, so the container's total memory use is the maximum JVM heap size plus the size of this cache.
stores.hdfs-write-vpnsessiondatares.container.cache.size.bytes=2000
# The amount of memory (in bytes) that LevelDB uses for buffering writes before they are written to disk,
stores.hdfs-write-vpnsessiondatares.container.write.buffer.size.bytes=1000

# top VPN scored session
stores.hdfs-write-vpnsessiondatares_top.factory=org.apache.samza.storage.kv.RocksDbKeyValueStorageEngineFactory
stores.hdfs-write-vpnsessiondatares_top.changelog=kafka.hdfs-write-vpnsessiondatares_top-changelog
stores.hdfs-write-vpnsessiondatares_top.changelog.replication.factor=1
stores.hdfs-write-vpnsessiondatares_top.key.serde=string
stores.hdfs-write-vpnsessiondatares_top.msg.serde=timebarrier
# This property is set to the number of key/value pairs that should be kept in this in-memory buffer, per task instance. The number cannot be greater than stores.*.object.cache.size.
stores.hdfs-write-vpnsessiondatares_top.write.batch.size=25
# This property determines the number of objects to keep in Samza's cache, per task instance. This same cache is also used for write buffering (see stores.*.write.batch.size). A value of 0 disables all caching and batching.
stores.hdfs-write-vpnsessiondatares_top.object.cache.size=100
# The size of LevelDB's block cache in bytes, per container. Note that this is an off-heap memory allocation, so the container's total memory use is the maximum JVM heap size plus the size of this cache.
stores.hdfs-write-vpnsessiondatares_top.container.cache.size.bytes=2000
# The amount of memory (in bytes) that LevelDB uses for buffering writes before they are written to disk,
stores.hdfs-write-vpnsessiondatares_top.container.write.buffer.size.bytes=1000


#############
# crmsf
fortscale.events.entry.name.crmsf_enriched_HDFSWriterStreamTask=crmsf_enriched_HDFSWriterStreamTask
fortscale.events.entry.crmsf_enriched_HDFSWriterStreamTask.data.source=crmsf
fortscale.events.entry.crmsf_enriched_HDFSWriterStreamTask.last.state=VpnEnrichTask
fortscale.events.entry.crmsf_enriched_HDFSWriterStreamTask.output.topics=fortscale-generic-data-access-enriched-after-write
fortscale.events.entry.crmsf_enriched_HDFSWriterStreamTask.bdp.output.topics=
fortscale.events.entry.crmsf_enriched_HDFSWriterStreamTask.timestamp.field=${impala.data.crmsf.table.field.epochtime}
fortscale.events.entry.crmsf_enriched_HDFSWriterStreamTask.username.field=normalized_username
fortscale.events.entry.crmsf_enriched_HDFSWriterStreamTask.fields=${impala.enricheddata.crmsf.table.fields}
fortscale.events.entry.crmsf_enriched_HDFSWriterStreamTask.separator=${impala.enricheddata.crmsf.table.delimiter}
fortscale.events.entry.crmsf_enriched_HDFSWriterStreamTask.hdfs.root=${hdfs.user.enricheddata.crmsf.path}
fortscale.events.entry.crmsf_enriched_HDFSWriterStreamTask.file.name=${hdfs.enricheddata.crmsf.file.name}
fortscale.events.entry.crmsf_enriched_HDFSWriterStreamTask.table.name=${impala.enricheddata.crmsf.table.name}
fortscale.events.entry.crmsf_enriched_HDFSWriterStreamTask.partition.strategy=${impala.enricheddata.crmsf.table.partition.type}
fortscale.events.entry.crmsf_enriched_HDFSWriterStreamTask.split.strategy=fortscale.utils.hdfs.split.DailyFileSplitStrategy
fortscale.events.entry.crmsf_enriched_HDFSWriterStreamTask.discriminator.fields=
# Buffer no more than 10000 events before flushing to HDFS
fortscale.events.entry.crmsf_enriched_HDFSWriterStreamTask.events.flush.threshold=10000
stores.hdfs-write-crmsfenriched.factory=org.apache.samza.storage.kv.RocksDbKeyValueStorageEngineFactory
stores.hdfs-write-crmsfenriched.changelog=kafka.hdfs-write-crmsfenriched-changelog
stores.hdfs-write-crmsfenriched.changelog.replication.factor=1
stores.hdfs-write-crmsfenriched.key.serde=string
stores.hdfs-write-crmsfenriched.msg.serde=timebarrier
# This property is set to the number of key/value pairs that should be kept in this in-memory buffer, per task instance. The number cannot be greater than stores.*.object.cache.size.
stores.hdfs-write-crmsfenriched.write.batch.size=25
# This property determines the number of objects to keep in Samza's cache, per task instance. This same cache is also used for write buffering (see stores.*.write.batch.size). A value of 0 disables all caching and batching.
stores.hdfs-write-crmsfenriched.object.cache.size=100
# The size of LevelDB's block cache in bytes, per container. Note that this is an off-heap memory allocation, so the container's total memory use is the maximum JVM heap size plus the size of this cache.
stores.hdfs-write-crmsfenriched.container.cache.size.bytes=2000
# The amount of memory (in bytes) that LevelDB uses for buffering writes before they are written to disk.
stores.hdfs-write-crmsfenriched.container.write.buffer.size.bytes=1000


# CRMSF scored
fortscale.events.entry.name.crmsf_scored_HDFSWriterStreamTask=crmsf_scored_HDFSWriterStreamTask
fortscale.events.entry.crmsf_scored_HDFSWriterStreamTask.data.source=crmsf
fortscale.events.entry.crmsf_scored_HDFSWriterStreamTask.last.state=MultipleEventsPrevalenceModelStreamTask
fortscale.events.entry.crmsf_scored_HDFSWriterStreamTask.output.topics=fortscale-generic-data-access-score-after-write,fortscale-generic-data-access-event-score-from-hdfs
fortscale.events.entry.crmsf_scored_HDFSWriterStreamTask.bdp.output.topics=fortscale-generic-data-access-score-after-write
fortscale.events.entry.crmsf_scored_HDFSWriterStreamTask.timestamp.field=${impala.data.crmsf.table.field.epochtime}
fortscale.events.entry.crmsf_scored_HDFSWriterStreamTask.username.field=${impala.table.fields.normalized.username}
fortscale.events.entry.crmsf_scored_HDFSWriterStreamTask.discriminator.fields=
fortscale.events.entry.crmsf_scored_HDFSWriterStreamTask.fields=${impala.score.crmsf.table.fields}
fortscale.events.entry.crmsf_scored_HDFSWriterStreamTask.separator=${impala.score.crmsf.table.delimiter}
fortscale.events.entry.crmsf_scored_HDFSWriterStreamTask.hdfs.root=${hdfs.user.processeddata.crmsf.path}
fortscale.events.entry.crmsf_scored_HDFSWriterStreamTask.table.name=${impala.score.crmsf.table.name}
fortscale.events.entry.crmsf_scored_HDFSWriterStreamTask.file.name=${impala.score.crmsf.table.name}.csv
fortscale.events.entry.crmsf_scored_HDFSWriterStreamTask.partition.strategy=${impala.score.crmsf.table.partition.type}
fortscale.events.entry.crmsf_scored_HDFSWriterStreamTask.split.strategy=fortscale.utils.hdfs.split.DailyFileSplitStrategy
# Buffer no more than 10000 events before flushing to HDFS
fortscale.events.entry.crmsf_scored_HDFSWriterStreamTask.events.flush.threshold=10000

stores.hdfs-write-crmsfscore.factory=org.apache.samza.storage.kv.RocksDbKeyValueStorageEngineFactory
stores.hdfs-write-crmsfscore.changelog=kafka.hdfs-write-crmsfscore-changelog
stores.hdfs-write-crmsfscore.changelog.replication.factor=1
stores.hdfs-write-crmsfscore.key.serde=string
stores.hdfs-write-crmsfscore.msg.serde=timebarrier
# This property is set to the number of key/value pairs that should be kept in this in-memory buffer, per task instance. The number cannot be greater than stores.*.object.cache.size.
stores.hdfs-write-crmsfscore.write.batch.size=25
# This property determines the number of objects to keep in Samza's cache, per task instance. This same cache is also used for write buffering (see stores.*.write.batch.size). A value of 0 disables all caching and batching.
stores.hdfs-write-crmsfscore.object.cache.size=100
# The size of LevelDB's block cache in bytes, per container. Note that this is an off-heap memory allocation, so the container's total memory use is the maximum JVM heap size plus the size of this cache.
stores.hdfs-write-crmsfscore.container.cache.size.bytes=2000
# The amount of memory (in bytes) that LevelDB uses for buffering writes before they are written to disk,
stores.hdfs-write-crmsfscore.container.write.buffer.size.bytes=1000

# top CRMSF scored
fortscale.events.entry.name.crmsf_top_scored_HDFSWriterStreamTask=crmsf_top_scored_HDFSWriterStreamTask
fortscale.events.entry.crmsf_top_scored_HDFSWriterStreamTask.data.source=crmsf
fortscale.events.entry.crmsf_top_scored_HDFSWriterStreamTask.last.state=MultipleEventsPrevalenceModelStreamTask
fortscale.events.entry.crmsf_top_scored_HDFSWriterStreamTask.timestamp.field=${impala.data.crmsf.table.field.epochtime}
fortscale.events.entry.crmsf_top_scored_HDFSWriterStreamTask.username.field=${impala.table.fields.normalized.username}
fortscale.events.entry.crmsf_top_scored_HDFSWriterStreamTask.discriminator.fields=
fortscale.events.entry.crmsf_top_scored_HDFSWriterStreamTask.fields=${impala.score.crmsf.table.fields}
fortscale.events.entry.crmsf_top_scored_HDFSWriterStreamTask.separator=${impala.score.crmsf.top.table.delimiter}
fortscale.events.entry.crmsf_top_scored_HDFSWriterStreamTask.hdfs.root=${hdfs.user.processeddata.crmsf.top.path}
fortscale.events.entry.crmsf_top_scored_HDFSWriterStreamTask.table.name=${impala.score.crmsf.top.table.name}
fortscale.events.entry.crmsf_top_scored_HDFSWriterStreamTask.file.name=${impala.score.crmsf.top.table.name}.csv
fortscale.events.entry.crmsf_top_scored_HDFSWriterStreamTask.partition.strategy=${impala.score.crmsf.top.table.partition.type}
fortscale.events.entry.crmsf_top_scored_HDFSWriterStreamTask.split.strategy=fortscale.utils.hdfs.split.DailyFileSplitStrategy
# Buffer no more than 10000 events before flushing to HDFS
fortscale.events.entry.crmsf_top_scored_HDFSWriterStreamTask.events.flush.threshold=10000
#Filter configuration moved into streaming-HdfsWriterTask-context.xml

fortscale.events.entry.crmsf_top_scored_HDFSWriterStreamTask.filters=gdsScoredTopScoreFilter

stores.hdfs-write-crmsfscore_top.factory=org.apache.samza.storage.kv.RocksDbKeyValueStorageEngineFactory
stores.hdfs-write-crmsfscore_top.changelog=kafka.hdfs-write-crmsfscore_top-changelog
stores.hdfs-write-crmsfscore_top.changelog.replication.factor=1
stores.hdfs-write-crmsfscore_top.key.serde=string
stores.hdfs-write-crmsfscore_top.msg.serde=timebarrier
# This property is set to the number of key/value pairs that should be kept in this in-memory buffer, per task instance. The number cannot be greater than stores.*.object.cache.size.
stores.hdfs-write-crmsfscore_top.write.batch.size=25
# This property determines the number of objects to keep in Samza's cache, per task instance. This same cache is also used for write buffering (see stores.*.write.batch.size). A value of 0 disables all caching and batching.
stores.hdfs-write-crmsfscore_top.object.cache.size=100
# The size of LevelDB's block cache in bytes, per container. Note that this is an off-heap memory allocation, so the container's total memory use is the maximum JVM heap size plus the size of this cache.
stores.hdfs-write-crmsfscore_top.container.cache.size.bytes=2000
# The amount of memory (in bytes) that LevelDB uses for buffering writes before they are written to disk,
stores.hdfs-write-crmsfscore_top.container.write.buffer.size.bytes=1000


# wame
fortscale.events.entry.name.wame_enriched_HDFSWriterStreamTask=wame_enriched_HDFSWriterStreamTask
fortscale.events.entry.wame_enriched_HDFSWriterStreamTask.data.source=wame
fortscale.events.entry.wame_enriched_HDFSWriterStreamTask.last.state=UsernameNormalizationAndTaggingTask
fortscale.events.entry.wame_enriched_HDFSWriterStreamTask.output.topics=fortscale-generic-data-access-enriched-after-write
fortscale.events.entry.wame_enriched_HDFSWriterStreamTask.bdp.output.topics=
fortscale.events.entry.wame_enriched_HDFSWriterStreamTask.timestamp.field=${impala.data.wame.table.field.epochtime}
fortscale.events.entry.wame_enriched_HDFSWriterStreamTask.username.field=normalized_username
fortscale.events.entry.wame_enriched_HDFSWriterStreamTask.fields=${impala.enricheddata.wame.table.fields}
fortscale.events.entry.wame_enriched_HDFSWriterStreamTask.separator=${impala.enricheddata.wame.table.delimiter}
fortscale.events.entry.wame_enriched_HDFSWriterStreamTask.hdfs.root=${hdfs.user.enricheddata.wame.path}
fortscale.events.entry.wame_enriched_HDFSWriterStreamTask.file.name=${hdfs.enricheddata.wame.file.name}
fortscale.events.entry.wame_enriched_HDFSWriterStreamTask.table.name=${impala.enricheddata.wame.table.name}
fortscale.events.entry.wame_enriched_HDFSWriterStreamTask.partition.strategy=${impala.enricheddata.wame.table.partition.type}
fortscale.events.entry.wame_enriched_HDFSWriterStreamTask.split.strategy=fortscale.utils.hdfs.split.DailyFileSplitStrategy
fortscale.events.entry.wame_enriched_HDFSWriterStreamTask.discriminator.fields=action_type
# Buffer no more than 10000 events before flushing to HDFS
fortscale.events.entry.wame_enriched_HDFSWriterStreamTask.events.flush.threshold=10000
stores.hdfs-write-wameenriched.factory=org.apache.samza.storage.kv.RocksDbKeyValueStorageEngineFactory
stores.hdfs-write-wameenriched.changelog=kafka.hdfs-write-wameenriched-changelog
stores.hdfs-write-wameenriched.changelog.replication.factor=1
stores.hdfs-write-wameenriched.key.serde=string
stores.hdfs-write-wameenriched.msg.serde=timebarrier
# This property is set to the number of key/value pairs that should be kept in this in-memory buffer, per task instance. The number cannot be greater than stores.*.object.cache.size.
stores.hdfs-write-wameenriched.write.batch.size=25
# This property determines the number of objects to keep in Samza's cache, per task instance. This same cache is also used for write buffering (see stores.*.write.batch.size). A value of 0 disables all caching and batching.
stores.hdfs-write-wameenriched.object.cache.size=100
# The size of LevelDB's block cache in bytes, per container. Note that this is an off-heap memory allocation, so the container's total memory use is the maximum JVM heap size plus the size of this cache.
stores.hdfs-write-wameenriched.container.cache.size.bytes=2000
# The amount of memory (in bytes) that LevelDB uses for buffering writes before they are written to disk.
stores.hdfs-write-wameenriched.container.write.buffer.size.bytes=1000
#############


# WAME scored
fortscale.events.entry.name.wame_scored_HDFSWriterStreamTask=wame_scored_HDFSWriterStreamTask
fortscale.events.entry.wame_scored_HDFSWriterStreamTask.data.source=wame
fortscale.events.entry.wame_scored_HDFSWriterStreamTask.last.state=MultipleEventsPrevalenceModelStreamTask
fortscale.events.entry.wame_scored_HDFSWriterStreamTask.output.topics=fortscale-generic-data-access-score-after-write,fortscale-generic-data-access-event-score-from-hdfs
fortscale.events.entry.wame_scored_HDFSWriterStreamTask.bdp.output.topics=fortscale-generic-data-access-score-after-write
fortscale.events.entry.wame_scored_HDFSWriterStreamTask.timestamp.field=${impala.data.wame.table.field.epochtime}
fortscale.events.entry.wame_scored_HDFSWriterStreamTask.username.field=${impala.table.fields.normalized.username}
fortscale.events.entry.wame_scored_HDFSWriterStreamTask.discriminator.fields=action_type
fortscale.events.entry.wame_scored_HDFSWriterStreamTask.fields=${impala.score.wame.table.fields}
fortscale.events.entry.wame_scored_HDFSWriterStreamTask.separator=${impala.score.wame.table.delimiter}
fortscale.events.entry.wame_scored_HDFSWriterStreamTask.hdfs.root=${hdfs.user.processeddata.wame.path}
fortscale.events.entry.wame_scored_HDFSWriterStreamTask.table.name=${impala.score.wame.table.name}
fortscale.events.entry.wame_scored_HDFSWriterStreamTask.file.name=${impala.score.wame.table.name}.csv
fortscale.events.entry.wame_scored_HDFSWriterStreamTask.partition.strategy=${impala.score.wame.table.partition.type}
fortscale.events.entry.wame_scored_HDFSWriterStreamTask.split.strategy=fortscale.utils.hdfs.split.DailyFileSplitStrategy
# Buffer no more than 10000 events before flushing to HDFS
fortscale.events.entry.wame_scored_HDFSWriterStreamTask.events.flush.threshold=10000

stores.hdfs-write-wamescore.factory=org.apache.samza.storage.kv.RocksDbKeyValueStorageEngineFactory
stores.hdfs-write-wamescore.changelog=kafka.hdfs-write-wamescore-changelog
stores.hdfs-write-wamescore.changelog.replication.factor=1

stores.hdfs-write-wamescore.key.serde=string
stores.hdfs-write-wamescore.msg.serde=timebarrier
# This property is set to the number of key/value pairs that should be kept in this in-memory buffer, per task instance. The number cannot be greater than stores.*.object.cache.size.
stores.hdfs-write-wamescore.write.batch.size=25
# This property determines the number of objects to keep in Samza's cache, per task instance. This same cache is also used for write buffering (see stores.*.write.batch.size). A value of 0 disables all caching and batching.
stores.hdfs-write-wamescore.object.cache.size=100
# The size of LevelDB's block cache in bytes, per container. Note that this is an off-heap memory allocation, so the container's total memory use is the maximum JVM heap size plus the size of this cache.
stores.hdfs-write-wamescore.container.cache.size.bytes=2000
# The amount of memory (in bytes) that LevelDB uses for buffering writes before they are written to disk,
stores.hdfs-write-wamescore.container.write.buffer.size.bytes=1000

# top WAME scored
fortscale.events.entry.name.wame_top_scored_HDFSWriterStreamTask=wame_top_scored_HDFSWriterStreamTask
fortscale.events.entry.wame_top_scored_HDFSWriterStreamTask.data.source=wame
fortscale.events.entry.wame_top_scored_HDFSWriterStreamTask.last.state=MultipleEventsPrevalenceModelStreamTask
fortscale.events.entry.wame_top_scored_HDFSWriterStreamTask.timestamp.field=${impala.data.wame.table.field.epochtime}
fortscale.events.entry.wame_top_scored_HDFSWriterStreamTask.username.field=${impala.table.fields.normalized.username}
fortscale.events.entry.wame_top_scored_HDFSWriterStreamTask.discriminator.fields=action_type
fortscale.events.entry.wame_top_scored_HDFSWriterStreamTask.fields=${impala.score.wame.table.fields}
fortscale.events.entry.wame_top_scored_HDFSWriterStreamTask.separator=${impala.score.wame.top.table.delimiter}
fortscale.events.entry.wame_top_scored_HDFSWriterStreamTask.hdfs.root=${hdfs.user.processeddata.wame.top.path}
fortscale.events.entry.wame_top_scored_HDFSWriterStreamTask.table.name=${impala.score.wame.top.table.name}
fortscale.events.entry.wame_top_scored_HDFSWriterStreamTask.file.name=${impala.score.wame.top.table.name}.csv
fortscale.events.entry.wame_top_scored_HDFSWriterStreamTask.partition.strategy=${impala.score.wame.top.table.partition.type}
fortscale.events.entry.wame_top_scored_HDFSWriterStreamTask.split.strategy=fortscale.utils.hdfs.split.DailyFileSplitStrategy
# Buffer no more than 10000 events before flushing to HDFS
fortscale.events.entry.wame_top_scored_HDFSWriterStreamTask.events.flush.threshold=10000
#Filter configuration moved into streaming-HdfsWriterTask-context.xml
fortscale.events.entry.wame_top_scored_HDFSWriterStreamTask.filters=gdsScoredTopScoreFilter

stores.hdfs-write-wamescore_top.factory=org.apache.samza.storage.kv.RocksDbKeyValueStorageEngineFactory
stores.hdfs-write-wamescore_top.changelog=kafka.hdfs-write-wamescore_top-changelog
stores.hdfs-write-wamescore_top.changelog.replication.factor=1
stores.hdfs-write-wamescore_top.key.serde=string
stores.hdfs-write-wamescore_top.msg.serde=timebarrier
# This property is set to the number of key/value pairs that should be kept in this in-memory buffer, per task instance. The number cannot be greater than stores.*.object.cache.size.
stores.hdfs-write-wamescore_top.write.batch.size=25
# This property determines the number of objects to keep in Samza's cache, per task instance. This same cache is also used for write buffering (see stores.*.write.batch.size). A value of 0 disables all caching and batching.
stores.hdfs-write-wamescore_top.object.cache.size=100
# The size of LevelDB's block cache in bytes, per container. Note that this is an off-heap memory allocation, so the container's total memory use is the maximum JVM heap size plus the size of this cache.
stores.hdfs-write-wamescore_top.container.cache.size.bytes=2000
# The amount of memory (in bytes) that LevelDB uses for buffering writes before they are written to disk,
stores.hdfs-write-wamescore_top.container.write.buffer.size.bytes=1000

# ntlm
fortscale.events.entry.name.ntlm_enriched_HDFSWriterStreamTask=ntlm_enriched_HDFSWriterStreamTask
fortscale.events.entry.ntlm_enriched_HDFSWriterStreamTask.data.source=ntlm
fortscale.events.entry.ntlm_enriched_HDFSWriterStreamTask.last.state=UsernameNormalizationAndTaggingTask
fortscale.events.entry.ntlm_enriched_HDFSWriterStreamTask.output.topics=fortscale-generic-data-access-enriched-after-write
fortscale.events.entry.ntlm_enriched_HDFSWriterStreamTask.bdp.output.topics=
fortscale.events.entry.ntlm_enriched_HDFSWriterStreamTask.timestamp.field=${impala.data.ntlm.table.field.epochtime}
fortscale.events.entry.ntlm_enriched_HDFSWriterStreamTask.username.field=normalized_username
fortscale.events.entry.ntlm_enriched_HDFSWriterStreamTask.fields=${impala.enricheddata.ntlm.table.fields}
fortscale.events.entry.ntlm_enriched_HDFSWriterStreamTask.separator=${impala.enricheddata.ntlm.table.delimiter}
fortscale.events.entry.ntlm_enriched_HDFSWriterStreamTask.hdfs.root=${hdfs.user.enricheddata.ntlm.path}
fortscale.events.entry.ntlm_enriched_HDFSWriterStreamTask.file.name=${hdfs.enricheddata.ntlm.file.name}
fortscale.events.entry.ntlm_enriched_HDFSWriterStreamTask.table.name=${impala.enricheddata.ntlm.table.name}
fortscale.events.entry.ntlm_enriched_HDFSWriterStreamTask.partition.strategy=${impala.enricheddata.ntlm.table.partition.type}
fortscale.events.entry.ntlm_enriched_HDFSWriterStreamTask.split.strategy=fortscale.utils.hdfs.split.DailyFileSplitStrategy
fortscale.events.entry.ntlm_enriched_HDFSWriterStreamTask.discriminator.fields=
# Buffer no more than 10000 events before flushing to HDFS
fortscale.events.entry.ntlm_enriched_HDFSWriterStreamTask.events.flush.threshold=10000
stores.hdfs-write-ntlmenriched.factory=org.apache.samza.storage.kv.RocksDbKeyValueStorageEngineFactory
stores.hdfs-write-ntlmenriched.changelog=kafka.hdfs-write-ntlmenriched-changelog
stores.hdfs-write-ntlmenriched.changelog.replication.factor=1
stores.hdfs-write-ntlmenriched.key.serde=string
stores.hdfs-write-ntlmenriched.msg.serde=timebarrier
# This property is set to the number of key/value pairs that should be kept in this in-memory buffer, per task instance. The number cannot be greater than stores.*.object.cache.size.
stores.hdfs-write-ntlmenriched.write.batch.size=25
# This property determines the number of objects to keep in Samza's cache, per task instance. This same cache is also used for write buffering (see stores.*.write.batch.size). A value of 0 disables all caching and batching.
stores.hdfs-write-ntlmenriched.object.cache.size=100
# The size of LevelDB's block cache in bytes, per container. Note that this is an off-heap memory allocation, so the container's total memory use is the maximum JVM heap size plus the size of this cache.
stores.hdfs-write-ntlmenriched.container.cache.size.bytes=2000
# The amount of memory (in bytes) that LevelDB uses for buffering writes before they are written to disk.
stores.hdfs-write-ntlmenriched.container.write.buffer.size.bytes=1000

# NTLM scored
fortscale.events.entry.name.ntlm_scored_HDFSWriterStreamTask=ntlm_scored_HDFSWriterStreamTask
fortscale.events.entry.ntlm_scored_HDFSWriterStreamTask.data.source=ntlm
fortscale.events.entry.ntlm_scored_HDFSWriterStreamTask.last.state=MultipleEventsPrevalenceModelStreamTask
fortscale.events.entry.ntlm_scored_HDFSWriterStreamTask.output.topics=fortscale-generic-data-access-score-after-write,fortscale-generic-data-access-event-score-from-hdfs
fortscale.events.entry.ntlm_scored_HDFSWriterStreamTask.bdp.output.topics=fortscale-generic-data-access-score-after-write
fortscale.events.entry.ntlm_scored_HDFSWriterStreamTask.timestamp.field=${impala.data.ntlm.table.field.epochtime}
fortscale.events.entry.ntlm_scored_HDFSWriterStreamTask.username.field=${impala.table.fields.normalized.username}
fortscale.events.entry.ntlm_scored_HDFSWriterStreamTask.discriminator.fields=
fortscale.events.entry.ntlm_scored_HDFSWriterStreamTask.fields=${impala.score.ntlm.table.fields}
fortscale.events.entry.ntlm_scored_HDFSWriterStreamTask.separator=${impala.score.ntlm.table.delimiter}
fortscale.events.entry.ntlm_scored_HDFSWriterStreamTask.hdfs.root=${hdfs.user.processeddata.ntlm.path}
fortscale.events.entry.ntlm_scored_HDFSWriterStreamTask.table.name=${impala.score.ntlm.table.name}
fortscale.events.entry.ntlm_scored_HDFSWriterStreamTask.file.name=${impala.score.ntlm.table.name}.csv
fortscale.events.entry.ntlm_scored_HDFSWriterStreamTask.partition.strategy=${impala.score.ntlm.table.partition.type}
fortscale.events.entry.ntlm_scored_HDFSWriterStreamTask.split.strategy=fortscale.utils.hdfs.split.DailyFileSplitStrategy
# Buffer no more than 10000 events before flushing to HDFS
fortscale.events.entry.ntlm_scored_HDFSWriterStreamTask.events.flush.threshold=10000

stores.hdfs-write-ntlmscore.factory=org.apache.samza.storage.kv.RocksDbKeyValueStorageEngineFactory
stores.hdfs-write-ntlmscore.changelog=kafka.hdfs-write-ntlmscore-changelog
stores.hdfs-write-ntlmscore.changelog.replication.factor=1

stores.hdfs-write-ntlmscore.key.serde=string
stores.hdfs-write-ntlmscore.msg.serde=timebarrier
# This property is set to the number of key/value pairs that should be kept in this in-memory buffer, per task instance. The number cannot be greater than stores.*.object.cache.size.
stores.hdfs-write-ntlmscore.write.batch.size=25
# This property determines the number of objects to keep in Samza's cache, per task instance. This same cache is also used for write buffering (see stores.*.write.batch.size). A value of 0 disables all caching and batching.
stores.hdfs-write-ntlmscore.object.cache.size=100
# The size of LevelDB's block cache in bytes, per container. Note that this is an off-heap memory allocation, so the container's total memory use is the maximum JVM heap size plus the size of this cache.
stores.hdfs-write-ntlmscore.container.cache.size.bytes=2000
# The amount of memory (in bytes) that LevelDB uses for buffering writes before they are written to disk,
stores.hdfs-write-ntlmscore.container.write.buffer.size.bytes=1000

# top NTLM scored
fortscale.events.entry.name.ntlm_top_scored_HDFSWriterStreamTask=ntlm_top_scored_HDFSWriterStreamTask
fortscale.events.entry.ntlm_top_scored_HDFSWriterStreamTask.data.source=ntlm
fortscale.events.entry.ntlm_top_scored_HDFSWriterStreamTask.last.state=MultipleEventsPrevalenceModelStreamTask
fortscale.events.entry.ntlm_top_scored_HDFSWriterStreamTask.timestamp.field=${impala.data.ntlm.table.field.epochtime}
fortscale.events.entry.ntlm_top_scored_HDFSWriterStreamTask.username.field=${impala.table.fields.normalized.username}
fortscale.events.entry.ntlm_top_scored_HDFSWriterStreamTask.discriminator.fields=
fortscale.events.entry.ntlm_top_scored_HDFSWriterStreamTask.fields=${impala.score.ntlm.table.fields}
fortscale.events.entry.ntlm_top_scored_HDFSWriterStreamTask.separator=${impala.score.ntlm.top.table.delimiter}
fortscale.events.entry.ntlm_top_scored_HDFSWriterStreamTask.hdfs.root=${hdfs.user.processeddata.ntlm.top.path}
fortscale.events.entry.ntlm_top_scored_HDFSWriterStreamTask.table.name=${impala.score.ntlm.top.table.name}
fortscale.events.entry.ntlm_top_scored_HDFSWriterStreamTask.file.name=${impala.score.ntlm.top.table.name}.csv
fortscale.events.entry.ntlm_top_scored_HDFSWriterStreamTask.partition.strategy=${impala.score.ntlm.top.table.partition.type}
fortscale.events.entry.ntlm_top_scored_HDFSWriterStreamTask.split.strategy=fortscale.utils.hdfs.split.DailyFileSplitStrategy
# Buffer no more than 10000 events before flushing to HDFS
fortscale.events.entry.ntlm_top_scored_HDFSWriterStreamTask.events.flush.threshold=10000
#Filter configuration moved into streaming-HdfsWriterTask-context.xml
fortscale.events.entry.ntlm_top_scored_HDFSWriterStreamTask.filters=gdsScoredTopScoreFilter

stores.hdfs-write-ntlmscore_top.factory=org.apache.samza.storage.kv.RocksDbKeyValueStorageEngineFactory
stores.hdfs-write-ntlmscore_top.changelog=kafka.hdfs-write-ntlmscore_top-changelog
stores.hdfs-write-ntlmscore_top.changelog.replication.factor=1
stores.hdfs-write-ntlmscore_top.key.serde=string
stores.hdfs-write-ntlmscore_top.msg.serde=timebarrier
# This property is set to the number of key/value pairs that should be kept in this in-memory buffer, per task instance. The number cannot be greater than stores.*.object.cache.size.
stores.hdfs-write-ntlmscore_top.write.batch.size=25
# This property determines the number of objects to keep in Samza's cache, per task instance. This same cache is also used for write buffering (see stores.*.write.batch.size). A value of 0 disables all caching and batching.
stores.hdfs-write-ntlmscore_top.object.cache.size=100
# The size of LevelDB's block cache in bytes, per container. Note that this is an off-heap memory allocation, so the container's total memory use is the maximum JVM heap size plus the size of this cache.
stores.hdfs-write-ntlmscore_top.container.cache.size.bytes=2000
# The amount of memory (in bytes) that LevelDB uses for buffering writes before they are written to disk,
stores.hdfs-write-ntlmscore_top.container.write.buffer.size.bytes=1000


#############

# gwame
fortscale.events.entry.name.gwame_enriched_HDFSWriterStreamTask=gwame_enriched_HDFSWriterStreamTask
fortscale.events.entry.gwame_enriched_HDFSWriterStreamTask.data.source=gwame
fortscale.events.entry.gwame_enriched_HDFSWriterStreamTask.last.state=UsernameNormalizationAndTaggingTask
fortscale.events.entry.gwame_enriched_HDFSWriterStreamTask.output.topics=fortscale-generic-data-access-enriched-after-write
fortscale.events.entry.gwame_enriched_HDFSWriterStreamTask.bdp.output.topics=
fortscale.events.entry.gwame_enriched_HDFSWriterStreamTask.timestamp.field=${impala.data.gwame.table.field.epochtime}
fortscale.events.entry.gwame_enriched_HDFSWriterStreamTask.username.field=normalized_username
fortscale.events.entry.gwame_enriched_HDFSWriterStreamTask.fields=${impala.enricheddata.gwame.table.fields}
fortscale.events.entry.gwame_enriched_HDFSWriterStreamTask.separator=${impala.enricheddata.gwame.table.delimiter}
fortscale.events.entry.gwame_enriched_HDFSWriterStreamTask.hdfs.root=${hdfs.user.enricheddata.gwame.path}
fortscale.events.entry.gwame_enriched_HDFSWriterStreamTask.file.name=${hdfs.enricheddata.gwame.file.name}
fortscale.events.entry.gwame_enriched_HDFSWriterStreamTask.table.name=${impala.enricheddata.gwame.table.name}
fortscale.events.entry.gwame_enriched_HDFSWriterStreamTask.partition.strategy=${impala.enricheddata.gwame.table.partition.type}
fortscale.events.entry.gwame_enriched_HDFSWriterStreamTask.split.strategy=fortscale.utils.hdfs.split.DailyFileSplitStrategy
fortscale.events.entry.gwame_enriched_HDFSWriterStreamTask.discriminator.fields=
# Buffer no more than 10000 events before flushing to HDFS
fortscale.events.entry.gwame_enriched_HDFSWriterStreamTask.events.flush.threshold=10000
stores.hdfs-write-gwameenriched.factory=org.apache.samza.storage.kv.RocksDbKeyValueStorageEngineFactory
stores.hdfs-write-gwameenriched.changelog=kafka.hdfs-write-gwameenriched-changelog
stores.hdfs-write-gwameenriched.changelog.replication.factor=1
stores.hdfs-write-gwameenriched.key.serde=string
stores.hdfs-write-gwameenriched.msg.serde=timebarrier
# This property is set to the number of key/value pairs that should be kept in this in-memory buffer, per task instance. The number cannot be greater than stores.*.object.cache.size.
stores.hdfs-write-gwameenriched.write.batch.size=25
# This property determines the number of objects to keep in Samza's cache, per task instance. This same cache is also used for write buffering (see stores.*.write.batch.size). A value of 0 disables all caching and batching.
stores.hdfs-write-gwameenriched.object.cache.size=100
# The size of LevelDB's block cache in bytes, per container. Note that this is an off-heap memory allocation, so the container's total memory use is the maximum JVM heap size plus the size of this cache.
stores.hdfs-write-gwameenriched.container.cache.size.bytes=2000
# The amount of memory (in bytes) that LevelDB uses for buffering writes before they are written to disk.
stores.hdfs-write-gwameenriched.container.write.buffer.size.bytes=1000


#############

# GWAME scored
fortscale.events.entry.name.gwame_scored_HDFSWriterStreamTask=gwame_scored_HDFSWriterStreamTask
fortscale.events.entry.gwame_scored_HDFSWriterStreamTask.data.source=gwame
fortscale.events.entry.gwame_scored_HDFSWriterStreamTask.last.state=MultipleEventsPrevalenceModelStreamTask
fortscale.events.entry.gwame_scored_HDFSWriterStreamTask.output.topics=fortscale-generic-data-access-score-after-write,fortscale-generic-data-access-event-score-from-hdfs
fortscale.events.entry.gwame_scored_HDFSWriterStreamTask.bdp.output.topics=fortscale-generic-data-access-score-after-write
fortscale.events.entry.gwame_scored_HDFSWriterStreamTask.timestamp.field=${impala.data.gwame.table.field.epochtime}
fortscale.events.entry.gwame_scored_HDFSWriterStreamTask.username.field=${impala.table.fields.normalized.username}
fortscale.events.entry.gwame_scored_HDFSWriterStreamTask.discriminator.fields=
fortscale.events.entry.gwame_scored_HDFSWriterStreamTask.fields=${impala.score.gwame.table.fields}
fortscale.events.entry.gwame_scored_HDFSWriterStreamTask.separator=${impala.score.gwame.table.delimiter}
fortscale.events.entry.gwame_scored_HDFSWriterStreamTask.hdfs.root=${hdfs.user.processeddata.gwame.path}
fortscale.events.entry.gwame_scored_HDFSWriterStreamTask.table.name=${impala.score.gwame.table.name}
fortscale.events.entry.gwame_scored_HDFSWriterStreamTask.file.name=${impala.score.gwame.table.name}.csv
fortscale.events.entry.gwame_scored_HDFSWriterStreamTask.partition.strategy=${impala.score.gwame.table.partition.type}
fortscale.events.entry.gwame_scored_HDFSWriterStreamTask.split.strategy=fortscale.utils.hdfs.split.DailyFileSplitStrategy
# Buffer no more than 10000 events before flushing to HDFS
fortscale.events.entry.gwame_scored_HDFSWriterStreamTask.events.flush.threshold=10000

stores.hdfs-write-gwamescore.factory=org.apache.samza.storage.kv.RocksDbKeyValueStorageEngineFactory
stores.hdfs-write-gwamescore.changelog=kafka.hdfs-write-gwamescore-changelog
stores.hdfs-write-gwamescore.changelog.replication.factor=1

stores.hdfs-write-gwamescore.key.serde=string
stores.hdfs-write-gwamescore.msg.serde=timebarrier
# This property is set to the number of key/value pairs that should be kept in this in-memory buffer, per task instance. The number cannot be greater than stores.*.object.cache.size.
stores.hdfs-write-gwamescore.write.batch.size=25
# This property determines the number of objects to keep in Samza's cache, per task instance. This same cache is also used for write buffering (see stores.*.write.batch.size). A value of 0 disables all caching and batching.
stores.hdfs-write-gwamescore.object.cache.size=100
# The size of LevelDB's block cache in bytes, per container. Note that this is an off-heap memory allocation, so the container's total memory use is the maximum JVM heap size plus the size of this cache.
stores.hdfs-write-gwamescore.container.cache.size.bytes=2000
# The amount of memory (in bytes) that LevelDB uses for buffering writes before they are written to disk,
stores.hdfs-write-gwamescore.container.write.buffer.size.bytes=1000

# top GWAME scored
fortscale.events.entry.name.gwame_top_scored_HDFSWriterStreamTask=gwame_top_scored_HDFSWriterStreamTask
fortscale.events.entry.gwame_top_scored_HDFSWriterStreamTask.data.source=gwame
fortscale.events.entry.gwame_top_scored_HDFSWriterStreamTask.last.state=MultipleEventsPrevalenceModelStreamTask
fortscale.events.entry.gwame_top_scored_HDFSWriterStreamTask.timestamp.field=${impala.data.gwame.table.field.epochtime}
fortscale.events.entry.gwame_top_scored_HDFSWriterStreamTask.username.field=${impala.table.fields.normalized.username}
fortscale.events.entry.gwame_top_scored_HDFSWriterStreamTask.discriminator.fields=
fortscale.events.entry.gwame_top_scored_HDFSWriterStreamTask.fields=${impala.score.gwame.table.fields}
fortscale.events.entry.gwame_top_scored_HDFSWriterStreamTask.separator=${impala.score.gwame.top.table.delimiter}
fortscale.events.entry.gwame_top_scored_HDFSWriterStreamTask.hdfs.root=${hdfs.user.processeddata.gwame.top.path}
fortscale.events.entry.gwame_top_scored_HDFSWriterStreamTask.table.name=${impala.score.gwame.top.table.name}
fortscale.events.entry.gwame_top_scored_HDFSWriterStreamTask.file.name=${impala.score.gwame.top.table.name}.csv
fortscale.events.entry.gwame_top_scored_HDFSWriterStreamTask.partition.strategy=${impala.score.gwame.top.table.partition.type}
fortscale.events.entry.gwame_top_scored_HDFSWriterStreamTask.split.strategy=fortscale.utils.hdfs.split.DailyFileSplitStrategy
# Buffer no more than 10000 events before flushing to HDFS
fortscale.events.entry.gwame_top_scored_HDFSWriterStreamTask.events.flush.threshold=10000
#Filter configuration moved into streaming-HdfsWriterTask-context.xml
fortscale.events.entry.gwame_top_scored_HDFSWriterStreamTask.filters=gdsScoredTopScoreFilter

stores.hdfs-write-gwamescore_top.factory=org.apache.samza.storage.kv.RocksDbKeyValueStorageEngineFactory
stores.hdfs-write-gwamescore_top.changelog=kafka.hdfs-write-gwamescore_top-changelog
stores.hdfs-write-gwamescore_top.changelog.replication.factor=1
stores.hdfs-write-gwamescore_top.key.serde=string
stores.hdfs-write-gwamescore_top.msg.serde=timebarrier
# This property is set to the number of key/value pairs that should be kept in this in-memory buffer, per task instance. The number cannot be greater than stores.*.object.cache.size.
stores.hdfs-write-gwamescore_top.write.batch.size=25
# This property determines the number of objects to keep in Samza's cache, per task instance. This same cache is also used for write buffering (see stores.*.write.batch.size). A value of 0 disables all caching and batching.
stores.hdfs-write-gwamescore_top.object.cache.size=100
# The size of LevelDB's block cache in bytes, per container. Note that this is an off-heap memory allocation, so the container's total memory use is the maximum JVM heap size plus the size of this cache.
stores.hdfs-write-gwamescore_top.container.cache.size.bytes=2000
# The amount of memory (in bytes) that LevelDB uses for buffering writes before they are written to disk,
stores.hdfs-write-gwamescore_top.container.write.buffer.size.bytes=1000


#############

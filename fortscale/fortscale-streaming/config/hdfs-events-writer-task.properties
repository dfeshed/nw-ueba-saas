# Job
job.factory.class=fortscale.streaming.GracefulShutdownLocalJobFactory
job.name=hdfs-events-writer-task

# Task
task.class=fortscale.streaming.task.HDFSWriterStreamTask
# input topics
task.inputs=kafka.fortscale-amt-geolocated,kafka.fortscale-amtsessions-event-score,kafka.fortscale-amt-event-score,kafka.fortscale-4768-enriched-events,kafka.fortscale-4769-computer-tagged-clustered,kafka.fortscale-4769-event-score,kafka.fortscale-ssh-computer-tagged-clustered,kafka.fortscale-ssh-event-score,kafka.fortscale-vpn-geolocated-session-updated,kafka.fortscale-vpn-event-score,kafka.fortscale-vpnsession-event-score
# flush the hdfs writers every 5 minutes (10*60*1000=300,000ms)
task.window.ms=300000


# Fortscale specific task config parameters
fortscale.context=classpath*:META-INF/spring/streaming-context.xml

# AMT enriched
fortscale.amt_enriched.input.topic=fortscale-amt-geolocated
fortscale.amt_enriched.timestamp.field=${impala.enricheddata.amt.table.field.epochtime}
fortscale.amt_enriched.username.field=${impala.enricheddata.amt.table.field.username}
fortscale.amt_enriched.discriminator.fields=${impala.enricheddata.amt.table.field.yid},${impala.enricheddata.amt.table.field.action_code},${impala.enricheddata.amt.table.field.uri},${impala.enricheddata.amt.table.field.action_string}
fortscale.amt_enriched.fields=${impala.enricheddata.amt.table.fields}
fortscale.amt_enriched.separator=${impala.enricheddata.amt.table.delimiter}
fortscale.amt_enriched.hdfs.root=${hdfs.user.enricheddata.amt.path}
fortscale.amt_enriched.table.name=${impala.enricheddata.amt.table.name}
fortscale.amt_enriched.file.name=${hdfs.enricheddata.amt.file.name}
fortscale.amt_enriched.partition.strategy=${impala.enricheddata.amt.table.partition.type}
fortscale.amt_enriched.split.strategy=fortscale.utils.hdfs.split.DailyFileSplitStrategy
# Buffer no more than 10000 events before flushing to HDFS
fortscale.amt_enriched.events.flush.threshold=10000

# AMT scored
fortscale.amt_scored.input.topic=fortscale-amt-event-score
fortscale.amt_scored.timestamp.field=date_time_unix
fortscale.amt_scored.username.field=normalized_username
fortscale.amt_scored.discriminator.fields=yid,action_code,uri
fortscale.amt_scored.fields=${impala.score.amt.table.fields}
fortscale.amt_scored.separator=|
fortscale.amt_scored.hdfs.root=/user/cloudera/processeddata/amtscores
fortscale.amt_scored.table.name=amtscores
fortscale.amt_scored.file.name=amtETL.csv
fortscale.amt_scored.partition.strategy=${impala.score.amt.table.partition.type}
fortscale.amt_scored.split.strategy=fortscale.utils.hdfs.split.DailyFileSplitStrategy
# Buffer no more than 10000 events before flushing to HDFS
fortscale.amt_scored.events.flush.threshold=10000

# AMT scored top
fortscale.amt_scored_top.input.topic=fortscale-amt-event-score
fortscale.amt_scored_top.timestamp.field=date_time_unix
fortscale.amt_scored_top.username.field=normalized_username
fortscale.amt_scored_top.discriminator.fields=yid,action_code,uri
fortscale.amt_scored_top.fields=${impala.score.amt_top.table.fields}
fortscale.amt_scored_top.separator=|
fortscale.amt_scored_top.hdfs.root=/user/cloudera/processeddata/amtscores_top
fortscale.amt_scored_top.table.name=amtscores_top
fortscale.amt_scored_top.file.name=amtETL.csv
fortscale.amt_scored_top.partition.strategy=${impala.score.amt_top.table.partition.type}
fortscale.amt_scored_top.split.strategy=fortscale.utils.hdfs.split.DailyFileSplitStrategy
# Buffer no more than 10000 events before flushing to HDFS
fortscale.amt_scored_top.events.flush.threshold=10000
fortscale.amt_scored_top.filters=score
fortscale.amt_scored_top.filter.score.class=fortscale.streaming.filters.EventScoreFilter
fortscale.amt_scored_top.filter.score.field=EventScore
fortscale.amt_scored_top.filter.score.threshold=50

# AMT scored session
fortscale.amt_scored_session.input.topic=fortscale-amtsessions-event-score
fortscale.amt_scored_session.timestamp.field=date_time_unix
fortscale.amt_scored_session.username.field=username
fortscale.amt_scored_session.discriminator.fields=
fortscale.amt_scored_session.fields=${impala.sessiondata.amt.table.fields}
fortscale.amt_scored_session.separator=|
fortscale.amt_scored_session.hdfs.root=/user/cloudera/processeddata/amt_session
fortscale.amt_scored_session.table.name=amtsessiondata
fortscale.amt_scored_session.file.name=amtSessions.csv
fortscale.amt_scored_session.partition.strategy=${impala.sessiondata.amt.table.partition.type}
fortscale.amt_scored_session.split.strategy=fortscale.utils.hdfs.split.DailyFileSplitStrategy
fortscale.amt_scored_session.feature.extractor.amt_host.class.json={"type":"priority_container_feature_extractor","featureExtractorList":[{"type":"event_feature_extractor","originalFieldName":"amt_host"},{"type":"event_feature_extractor","originalFieldName":"source_ip"}]}
# Buffer no more than 10000 events before flushing to HDFS
fortscale.amt_scored_session.events.flush.threshold=10000

# 4768 enriched
fortscale.4768_enriched.input.topic=fortscale-4768-enriched-events
fortscale.4768_enriched.timestamp.field=date_time_unix
fortscale.4768_enriched.username.field=normalized_username
fortscale.4768_enriched.discriminator.fields=recordNumber
fortscale.4768_enriched.fields=${impala.data.security.events.login.table.fields}
fortscale.4768_enriched.separator=|
fortscale.4768_enriched.hdfs.root=/user/cloudera/data/login
fortscale.4768_enriched.table.name=logindata
fortscale.4768_enriched.file.name=datalogindata.csv
fortscale.4768_enriched.partition.strategy=${impala.data.security.events.login.table.partition.type}
fortscale.4768_enriched.split.strategy=fortscale.utils.hdfs.split.DailyFileSplitStrategy
# Buffer no more than 10000 events before flushing to HDFS
fortscale.4768_enriched.events.flush.threshold=10000

# 4769 enriched
fortscale.4769_enriched.input.topic=fortscale-4769-computer-tagged-clustered
fortscale.4769_enriched.timestamp.field=${impala.enricheddata.security.events.table.field.date_time_unix}
fortscale.4769_enriched.username.field=${impala.enricheddata.security.events.table.field.normalized_username}
fortscale.4769_enriched.discriminator.fields=${impala.enricheddata.security.events.table.field.record_number}
fortscale.4769_enriched.fields=${impala.enricheddata.security.events.table.fields}
fortscale.4769_enriched.separator=${impala.enricheddata.security.events.table.delimiter}
fortscale.4769_enriched.hdfs.root=${hdfs.user.enricheddata.security.events.path}
fortscale.4769_enriched.table.name=${impala.enricheddata.security.events.table.name}
fortscale.4769_enriched.file.name=${hdfs.enricheddata.security.events.file.name}
fortscale.4769_enriched.partition.strategy=${impala.enricheddata.security.events.table.partition.type}
fortscale.4769_enriched.split.strategy=fortscale.utils.hdfs.split.DailyFileSplitStrategy
# Buffer no more than 10000 events before flushing to HDFS
fortscale.4769_enriched.events.flush.threshold=10000

# 4769 scored
fortscale.4769_scored.input.topic=fortscale-4769-event-score
fortscale.4769_scored.timestamp.field=date_time_unix
fortscale.4769_scored.username.field=normalized_username
fortscale.4769_scored.discriminator.fields=recordNumber
fortscale.4769_scored.fields=${impala.score.ldapauth.table.fields}
fortscale.4769_scored.separator=,
fortscale.4769_scored.hdfs.root=/user/cloudera/processeddata/authentication
fortscale.4769_scored.table.name=authenticationscores
fortscale.4769_scored.file.name=secData.csv
fortscale.4769_scored.partition.strategy=${impala.score.ldapauth.table.partition.type}
fortscale.4769_scored.split.strategy=fortscale.utils.hdfs.split.DailyFileSplitStrategy
# Buffer no more than 10000 events before flushing to HDFS
fortscale.4769_scored.events.flush.threshold=10000

# 4769 scored top
fortscale.4769_scored_top.input.topic=fortscale-4769-event-score
fortscale.4769_scored_top.timestamp.field=date_time_unix
fortscale.4769_scored_top.username.field=normalized_username
fortscale.4769_scored_top.discriminator.fields=recordNumber
fortscale.4769_scored_top.fields=${impala.score.ldapauth.table.fields}
fortscale.4769_scored_top.separator=,
fortscale.4769_scored_top.hdfs.root=/user/cloudera/processeddata/authentication_top
fortscale.4769_scored_top.table.name=authenticationscores_top
fortscale.4769_scored_top.file.name=secData.csv
fortscale.4769_scored_top.partition.strategy=${impala.score.ldapauth_top.table.partition.type}
fortscale.4769_scored_top.split.strategy=fortscale.utils.hdfs.split.DailyFileSplitStrategy
# Buffer no more than 10000 events before flushing to HDFS
fortscale.4769_scored_top.events.flush.threshold=10000
fortscale.4769_scored_top.filters=score
fortscale.4769_scored_top.filter.score.class=fortscale.streaming.filters.EventScoreFilter
fortscale.4769_scored_top.filter.score.field=eventscore
fortscale.4769_scored_top.filter.score.threshold=50

# SSH enriched
fortscale.ssh_enriched.input.topic=fortscale-ssh-computer-tagged-clustered
fortscale.ssh_enriched.timestamp.field=${impala.enricheddata.ssh.table.field.date_time_unix}
fortscale.ssh_enriched.username.field=${impala.enricheddata.ssh.table.field.normalized_username}
fortscale.ssh_enriched.discriminator.fields=${impala.enricheddata.ssh.table.field.target_machine}
fortscale.ssh_enriched.fields=${impala.enricheddata.ssh.table.fields}
fortscale.ssh_enriched.separator=${impala.enricheddata.ssh.table.delimiter}
fortscale.ssh_enriched.hdfs.root=${hdfs.user.enricheddata.ssh.path}
fortscale.ssh_enriched.table.name=${impala.enricheddata.ssh.table.name}
fortscale.ssh_enriched.file.name=${hdfs.enricheddata.ssh.file.name}
fortscale.ssh_enriched.partition.strategy=${impala.enricheddata.ssh.table.partition.type}
fortscale.ssh_enriched.split.strategy=fortscale.utils.hdfs.split.DailyFileSplitStrategy
# Buffer no more than 10000 events before flushing to HDFS
fortscale.ssh_enriched.events.flush.threshold=10000

# SSH scored
fortscale.ssh_scored.input.topic=fortscale-ssh-event-score
fortscale.ssh_scored.timestamp.field=date_time_unix
fortscale.ssh_scored.username.field=normalized_username
fortscale.ssh_scored.discriminator.fields=target_machine
fortscale.ssh_scored.fields=${impala.score.ssh.table.fields}
fortscale.ssh_scored.separator=,
fortscale.ssh_scored.hdfs.root=/user/cloudera/processeddata/sshscores
fortscale.ssh_scored.table.name=sshscores
fortscale.ssh_scored.file.name=sshETL.csv
fortscale.ssh_scored.partition.strategy=${impala.score.ssh.table.partition.type}
fortscale.ssh_scored.split.strategy=fortscale.utils.hdfs.split.DailyFileSplitStrategy
# Buffer no more than 10000 events before flushing to HDFS
fortscale.ssh_scored.events.flush.threshold=10000

# SSH scored top
fortscale.ssh_scored_top.input.topic=fortscale-ssh-event-score
fortscale.ssh_scored_top.timestamp.field=date_time_unix
fortscale.ssh_scored_top.username.field=normalized_username
fortscale.ssh_scored_top.discriminator.fields=target_machine
fortscale.ssh_scored_top.fields=${impala.score.ssh.table.fields}
fortscale.ssh_scored_top.separator=,
fortscale.ssh_scored_top.hdfs.root=/user/cloudera/processeddata/sshscores_top
fortscale.ssh_scored_top.table.name=sshscores_top
fortscale.ssh_scored_top.file.name=sshETL.csv
fortscale.ssh_scored_top.partition.strategy=${impala.score.ssh_top.table.partition.type}
fortscale.ssh_scored_top.split.strategy=fortscale.utils.hdfs.split.DailyFileSplitStrategy
# Buffer no more than 10000 events before flushing to HDFS
fortscale.ssh_scored_top.events.flush.threshold=10000
fortscale.ssh_scored_top.filters=score
fortscale.ssh_scored_top.filter.score.class=fortscale.streaming.filters.EventScoreFilter
fortscale.ssh_scored_top.filter.score.field=EventScore
fortscale.ssh_scored_top.filter.score.threshold=50

# VPN enriched
fortscale.vpn_enriched.input.topic=fortscale-vpn-geolocated-session-updated
fortscale.vpn_enriched.timestamp.field=${impala.enricheddata.vpn.table.field.date_time_unix}
fortscale.vpn_enriched.username.field=${impala.enricheddata.vpn.table.field.normalized_username}
fortscale.vpn_enriched.discriminator.fields=${impala.enricheddata.vpn.table.field.local_ip}
fortscale.vpn_enriched.fields=${impala.enricheddata.vpn.table.fields}
fortscale.vpn_enriched.separator=${impala.enricheddata.vpn.table.delimiter}
fortscale.vpn_enriched.hdfs.root=${hdfs.user.enricheddata.vpn.path}
fortscale.vpn_enriched.table.name=${impala.enricheddata.vpn.table.name}
fortscale.vpn_enriched.file.name=${hdfs.enricheddata.vpn.file.name}
fortscale.vpn_enriched.partition.strategy=${impala.enricheddata.vpn.table.partition.type}
fortscale.vpn_enriched.split.strategy=fortscale.utils.hdfs.split.DailyFileSplitStrategy
# Buffer no more than 10000 events before flushing to HDFS
fortscale.vpn_enriched.events.flush.threshold=10000

# VPN scored
fortscale.vpn_scored.input.topic=fortscale-vpn-event-score
fortscale.vpn_scored.timestamp.field=date_time_unix
fortscale.vpn_scored.username.field=normalized_username
fortscale.vpn_scored.discriminator.fields=local_ip
fortscale.vpn_scored.fields=${impala.score.vpn.table.fields}
fortscale.vpn_scored.separator=,
fortscale.vpn_scored.hdfs.root=/user/cloudera/processeddata/vpn
fortscale.vpn_scored.table.name=vpndatares
fortscale.vpn_scored.file.name=vpnETL.csv
fortscale.vpn_scored.partition.strategy=${impala.score.vpn.table.partition.type}
fortscale.vpn_scored.split.strategy=fortscale.utils.hdfs.split.DailyFileSplitStrategy
# Buffer no more than 10000 events before flushing to HDFS
fortscale.vpn_scored.events.flush.threshold=10000

# top VPN scored
fortscale.vpn_scored_top.input.topic=fortscale-vpn-event-score
fortscale.vpn_scored_top.timestamp.field=date_time_unix
fortscale.vpn_scored_top.username.field=normalized_username
fortscale.vpn_scored_top.discriminator.fields=local_ip
fortscale.vpn_scored_top.fields=${impala.score.vpn.table.fields}
fortscale.vpn_scored_top.separator=,
fortscale.vpn_scored_top.hdfs.root=/user/cloudera/processeddata/vpn_top
fortscale.vpn_scored_top.table.name=vpndatares_top
fortscale.vpn_scored_top.file.name=vpnETL.csv
fortscale.vpn_scored_top.partition.strategy=${impala.score.vpn_top.table.partition.type}
fortscale.vpn_scored_top.split.strategy=fortscale.utils.hdfs.split.DailyFileSplitStrategy
# Buffer no more than 10000 events before flushing to HDFS
fortscale.vpn_scored_top.events.flush.threshold=10000
fortscale.vpn_scored_top.filters=score
fortscale.vpn_scored_top.filter.score.class=fortscale.streaming.filters.EventScoreFilter
fortscale.vpn_scored_top.filter.score.field=eventScore
fortscale.vpn_scored_top.filter.score.threshold=50

# VPN scored session
fortscale.vpn_scored_session.input.topic=fortscale-vpnsession-event-score
fortscale.vpn_scored_session.timestamp.field=date_time_unix
fortscale.vpn_scored_session.username.field=normalized_username
fortscale.vpn_scored_session.discriminator.fields=local_ip
fortscale.vpn_scored_session.fields=${impala.score.vpn.session.table.fields}
fortscale.vpn_scored_session.separator=,
fortscale.vpn_scored_session.hdfs.root=/user/cloudera/processeddata/vpnsession
fortscale.vpn_scored_session.table.name=vpnsessiondatares
fortscale.vpn_scored_session.file.name=vpnETL.csv
fortscale.vpn_scored_session.partition.strategy=${impala.score.vpn.session.table.partition.type}
fortscale.vpn_scored_session.split.strategy=fortscale.utils.hdfs.split.DailyFileSplitStrategy
# Buffer no more than 10000 events before flushing to HDFS
fortscale.vpn_scored_session.events.flush.threshold=10000

# top VPN scored session
fortscale.vpn_scored_session_top.input.topic=fortscale-vpnsession-event-score
fortscale.vpn_scored_session_top.timestamp.field=date_time_unix
fortscale.vpn_scored_session_top.username.field=normalized_username
fortscale.vpn_scored_session_top.discriminator.fields=local_ip
fortscale.vpn_scored_session_top.fields=${impala.score.vpn.session.table.fields}
fortscale.vpn_scored_session_top.separator=,
fortscale.vpn_scored_session_top.hdfs.root=/user/cloudera/processeddata/vpnsession_top
fortscale.vpn_scored_session_top.table.name=vpnsessiondatares_top
fortscale.vpn_scored_session_top.file.name=vpnETL.csv
fortscale.vpn_scored_session_top.partition.strategy=${impala.score.vpn.session.top.table.partition.type}
fortscale.vpn_scored_session_top.split.strategy=fortscale.utils.hdfs.split.DailyFileSplitStrategy
# Buffer no more than 10000 events before flushing to HDFS
fortscale.vpn_scored_session_top.events.flush.threshold=10000
fortscale.vpn_scored_session_top.filters=score
fortscale.vpn_scored_session_top.filter.score.class=fortscale.streaming.filters.EventScoreFilter
fortscale.vpn_scored_session_top.filter.score.field=eventScore
fortscale.vpn_scored_session_top.filter.score.threshold=50


# Serializers
serializers.registry.json.class=org.apache.samza.serializers.JsonSerdeFactory
serializers.registry.string.class=org.apache.samza.serializers.StringSerdeFactory
serializers.registry.integer.class=org.apache.samza.serializers.IntegerSerdeFactory
serializers.registry.long.class=fortscale.streaming.serialization.LongSerdeFactory
serializers.registry.metrics.class=org.apache.samza.serializers.MetricsSnapshotSerdeFactory
serializers.registry.timebarrier.class=fortscale.streaming.serialization.UserTimeBarrierModelSerdeFactory

# Metric report every 60 seconds to a kafka topic called metrics and as a monitor report
metrics.reporter.snapshot.class=org.apache.samza.metrics.reporter.MetricsSnapshotReporterFactory
metrics.reporter.snapshot.stream=kafka.metrics
systems.kafka.streams.metrics.samza.msg.serde=metrics
metrics.reporters=snapshot

# Systems
systems.kafka.samza.factory=org.apache.samza.system.kafka.KafkaSystemFactory
systems.kafka.samza.msg.serde=string
systems.kafka.samza.offset.default=oldest
systems.kafka.consumer.zookeeper.connect=localhost:2181
systems.kafka.consumer.auto.offset.reset=smallest
systems.kafka.producer.metadata.broker.list=localhost:9092
systems.kafka.producer.producer.type=sync
systems.kafka.producer.retry.backoff.ms=10000
systems.kafka.producer.acks=1
systems.kafka.producer.reconnect.backoff.ms=10000
systems.kafka.producer.queue.buffering.max.ms=2000
systems.kafka.producer.batch.num.messages=1000

# Declare that we want our job's checkpoints to be enabled and written to Kafka
task.checkpoint.factory=org.apache.samza.checkpoint.kafka.KafkaCheckpointManagerFactory
task.checkpoint.replication.factor=1
task.checkpoint.system=kafka

# Key-value storage
####################

# amt_enriched
stores.hdfs-write-amtenriched.factory=org.apache.samza.storage.kv.KeyValueStorageEngineFactory
stores.hdfs-write-amtenriched.changelog=kafka.hdfs-write-vpnenriched-changelog
stores.hdfs-write-amtenriched.key.serde=string
stores.hdfs-write-amtenriched.msg.serde=timebarrier
# This property is set to the number of key/value pairs that should be kept in this in-memory buffer, per task instance. The number cannot be greater than stores.*.object.cache.size.
stores.hdfs-write-amtenriched.write.batch.size=25
# This property determines the number of objects to keep in Samza's cache, per task instance. This same cache is also used for write buffering (see stores.*.write.batch.size). A value of 0 disables all caching and batching.
stores.hdfs-write-amtenriched.object.cache.size=100
# The size of LevelDB's block cache in bytes, per container. Note that this is an off-heap memory allocation, so the container's total memory use is the maximum JVM heap size plus the size of this cache.
stores.hdfs-write-amtenriched.container.cache.size.bytes=2000
# The amount of memory (in bytes) that LevelDB uses for buffering writes before they are written to disk.
stores.hdfs-write-amtenriched.container.write.buffer.size.bytes=1000

# AMT scored
stores.hdfs-write-amtscores.factory=org.apache.samza.storage.kv.KeyValueStorageEngineFactory
stores.hdfs-write-amtscores.changelog=kafka.hdfs-write-amtscores-changelog
stores.hdfs-write-amtscores.key.serde=string
stores.hdfs-write-amtscores.msg.serde=timebarrier
# This property is set to the number of key/value pairs that should be kept in this in-memory buffer, per task instance. The number cannot be greater than stores.*.object.cache.size.
stores.hdfs-write-amtscores.write.batch.size=25
# This property determines the number of objects to keep in Samza's cache, per task instance. This same cache is also used for write buffering (see stores.*.write.batch.size). A value of 0 disables all caching and batching.
stores.hdfs-write-amtscores.object.cache.size=100
# The size of LevelDB's block cache in bytes, per container. Note that this is an off-heap memory allocation, so the container's total memory use is the maximum JVM heap size plus the size of this cache.
stores.hdfs-write-amtscores.container.cache.size.bytes=2000
# The amount of memory (in bytes) that LevelDB uses for buffering writes before they are written to disk.
stores.hdfs-write-amtscores.container.write.buffer.size.bytes=1000

# AMT scored top
stores.hdfs-write-amtscores_top.factory=org.apache.samza.storage.kv.KeyValueStorageEngineFactory
stores.hdfs-write-amtscores_top.changelog=kafka.hdfs-write-amtscores_top-changelog
stores.hdfs-write-amtscores_top.key.serde=string
stores.hdfs-write-amtscores_top.msg.serde=timebarrier
# This property is set to the number of key/value pairs that should be kept in this in-memory buffer, per task instance. The number cannot be greater than stores.*.object.cache.size.
stores.hdfs-write-amtscores_top.write.batch.size=25
# This property determines the number of objects to keep in Samza's cache, per task instance. This same cache is also used for write buffering (see stores.*.write.batch.size). A value of 0 disables all caching and batching.
stores.hdfs-write-amtscores_top.object.cache.size=100
# The size of LevelDB's block cache in bytes, per container. Note that this is an off-heap memory allocation, so the container's total memory use is the maximum JVM heap size plus the size of this cache.
stores.hdfs-write-amtscores_top.container.cache.size.bytes=2000
# The amount of memory (in bytes) that LevelDB uses for buffering writes before they are written to disk.
stores.hdfs-write-amtscores_top.container.write.buffer.size.bytes=1000

# AMT Scored session
stores.hdfs-write-amtsessiondata.factory=org.apache.samza.storage.kv.KeyValueStorageEngineFactory
stores.hdfs-write-amtsessiondata.changelog=kafka.hdfs-write-amtsessiondata-changelog
stores.hdfs-write-amtsessiondata.key.serde=string
stores.hdfs-write-amtsessiondata.msg.serde=timebarrier
# This property is set to the number of key/value pairs that should be kept in this in-memory buffer, per task instance. The number cannot be greater than stores.*.object.cache.size.
stores.hdfs-write-amtsessiondata.write.batch.size=25
# This property determines the number of objects to keep in Samza's cache, per task instance. This same cache is also used for write buffering (see stores.*.write.batch.size). A value of 0 disables all caching and batching.
stores.hdfs-write-amtsessiondata.object.cache.size=100
# The size of LevelDB's block cache in bytes, per container. Note that this is an off-heap memory allocation, so the container's total memory use is the maximum JVM heap size plus the size of this cache.
stores.hdfs-write-amtsessiondata.container.cache.size.bytes=2000
# The amount of memory (in bytes) that LevelDB uses for buffering writes before they are written to disk.
stores.hdfs-write-amtsessiondata.container.write.buffer.size.bytes=1000

# 4768 enriched
stores.hdfs-write-logindata.factory=org.apache.samza.storage.kv.KeyValueStorageEngineFactory
stores.hdfs-write-logindata.changelog=kafka.hdfs-write-logindata-changelog
stores.hdfs-write-logindata.key.serde=string
stores.hdfs-write-logindata.msg.serde=timebarrier
# This property is set to the number of key/value pairs that should be kept in this in-memory buffer, per task instance. The number cannot be greater than stores.*.object.cache.size.
stores.hdfs-write-logindata.write.batch.size=25
# This property determines the number of objects to keep in Samza's cache, per task instance. This same cache is also used for write buffering (see stores.*.write.batch.size). A value of 0 disables all caching and batching.
stores.hdfs-write-logindata.object.cache.size=100
# The size of LevelDB's block cache in bytes, per container. Note that this is an off-heap memory allocation, so the container's total memory use is the maximum JVM heap size plus the size of this cache.
stores.hdfs-write-logindata.container.cache.size.bytes=2000
# The amount of memory (in bytes) that LevelDB uses for buffering writes before they are written to disk,
stores.hdfs-write-logindata.container.write.buffer.size.bytes=1000

# 4769 enriched
stores.hdfs-write-4769enriched.factory=org.apache.samza.storage.kv.KeyValueStorageEngineFactory
stores.hdfs-write-4769enriched.changelog=kafka.hdfs-write-4769enriched-changelog
stores.hdfs-write-4769enriched.key.serde=string
stores.hdfs-write-4769enriched.msg.serde=timebarrier
# This property is set to the number of key/value pairs that should be kept in this in-memory buffer, per task instance. The number cannot be greater than stores.*.object.cache.size.
stores.hdfs-write-4769enriched.write.batch.size=25
# This property determines the number of objects to keep in Samza's cache, per task instance. This same cache is also used for write buffering (see stores.*.write.batch.size). A value of 0 disables all caching and batching.
stores.hdfs-write-4769enriched.object.cache.size=100
# The size of LevelDB's block cache in bytes, per container. Note that this is an off-heap memory allocation, so the container's total memory use is the maximum JVM heap size plus the size of this cache.
stores.hdfs-write-4769enriched.container.cache.size.bytes=2000
# The amount of memory (in bytes) that LevelDB uses for buffering writes before they are written to disk.
stores.hdfs-write-4769enriched.container.write.buffer.size.bytes=1000

# 4769 scored
stores.hdfs-write-authenticationscores.factory=org.apache.samza.storage.kv.KeyValueStorageEngineFactory
stores.hdfs-write-authenticationscores.changelog=kafka.hdfs-write-authenticationscores-changelog
stores.hdfs-write-authenticationscores.key.serde=string
stores.hdfs-write-authenticationscores.msg.serde=timebarrier
# This property is set to the number of key/value pairs that should be kept in this in-memory buffer, per task instance. The number cannot be greater than stores.*.object.cache.size.
stores.hdfs-write-authenticationscores.write.batch.size=25
# This property determines the number of objects to keep in Samza's cache, per task instance. This same cache is also used for write buffering (see stores.*.write.batch.size). A value of 0 disables all caching and batching.
stores.hdfs-write-authenticationscores.object.cache.size=100
# The size of LevelDB's block cache in bytes, per container. Note that this is an off-heap memory allocation, so the container's total memory use is the maximum JVM heap size plus the size of this cache.
stores.hdfs-write-authenticationscores.container.cache.size.bytes=2000
# The amount of memory (in bytes) that LevelDB uses for buffering writes before they are written to disk,
stores.hdfs-write-authenticationscores.container.write.buffer.size.bytes=1000

# 4769 scored top
stores.hdfs-write-authenticationscores_top.factory=org.apache.samza.storage.kv.KeyValueStorageEngineFactory
stores.hdfs-write-authenticationscores_top.changelog=kafka.hdfs-write-authenticationscores_top-changelog
stores.hdfs-write-authenticationscores_top.key.serde=string
stores.hdfs-write-authenticationscores_top.msg.serde=timebarrier
# This property is set to the number of key/value pairs that should be kept in this in-memory buffer, per task instance. The number cannot be greater than stores.*.object.cache.size.
stores.hdfs-write-authenticationscores_top.write.batch.size=25
# This property determines the number of objects to keep in Samza's cache, per task instance. This same cache is also used for write buffering (see stores.*.write.batch.size). A value of 0 disables all caching and batching.
stores.hdfs-write-authenticationscores_top.object.cache.size=100
# The size of LevelDB's block cache in bytes, per container. Note that this is an off-heap memory allocation, so the container's total memory use is the maximum JVM heap size plus the size of this cache.
stores.hdfs-write-authenticationscores_top.container.cache.size.bytes=2000
# The amount of memory (in bytes) that LevelDB uses for buffering writes before they are written to disk,
stores.hdfs-write-authenticationscores_top.container.write.buffer.size.bytes=1000

# SSH enriched
stores.hdfs-write-sshenriched.factory=org.apache.samza.storage.kv.KeyValueStorageEngineFactory
stores.hdfs-write-sshenriched.changelog=kafka.hdfs-write-sshenriched-changelog
stores.hdfs-write-sshenriched.key.serde=string
stores.hdfs-write-sshenriched.msg.serde=timebarrier
# This property is set to the number of key/value pairs that should be kept in this in-memory buffer, per task instance. The number cannot be greater than stores.*.object.cache.size.
stores.hdfs-write-sshenriched.write.batch.size=25
# This property determines the number of objects to keep in Samza's cache, per task instance. This same cache is also used for write buffering (see stores.*.write.batch.size). A value of 0 disables all caching and batching.
stores.hdfs-write-sshenriched.object.cache.size=100
# The size of LevelDB's block cache in bytes, per container. Note that this is an off-heap memory allocation, so the container's total memory use is the maximum JVM heap size plus the size of this cache.
stores.hdfs-write-sshenriched.container.cache.size.bytes=2000
# The amount of memory (in bytes) that LevelDB uses for buffering writes before they are written to disk.
stores.hdfs-write-sshenriched.container.write.buffer.size.bytes=1000

# SSH scored
stores.hdfs-write-sshscores.factory=org.apache.samza.storage.kv.KeyValueStorageEngineFactory
stores.hdfs-write-sshscores.changelog=kafka.hdfs-write-sshscores-changelog
stores.hdfs-write-sshscores.key.serde=string
stores.hdfs-write-sshscores.msg.serde=timebarrier
# This property is set to the number of key/value pairs that should be kept in this in-memory buffer, per task instance. The number cannot be greater than stores.*.object.cache.size.
stores.hdfs-write-sshscores.write.batch.size=25
# This property determines the number of objects to keep in Samza's cache, per task instance. This same cache is also used for write buffering (see stores.*.write.batch.size). A value of 0 disables all caching and batching.
stores.hdfs-write-sshscores.object.cache.size=100
# The size of LevelDB's block cache in bytes, per container. Note that this is an off-heap memory allocation, so the container's total memory use is the maximum JVM heap size plus the size of this cache.
stores.hdfs-write-sshscores.container.cache.size.bytes=2000
# The amount of memory (in bytes) that LevelDB uses for buffering writes before they are written to disk,
stores.hdfs-write-sshscores.container.write.buffer.size.bytes=1000

# SSH scored top
stores.hdfs-write-sshscores_top.factory=org.apache.samza.storage.kv.KeyValueStorageEngineFactory
stores.hdfs-write-sshscores_top.changelog=kafka.hdfs-write-sshscores_top-changelog
stores.hdfs-write-sshscores_top.key.serde=string
stores.hdfs-write-sshscores_top.msg.serde=timebarrier
# This property is set to the number of key/value pairs that should be kept in this in-memory buffer, per task instance. The number cannot be greater than stores.*.object.cache.size.
stores.hdfs-write-sshscores_top.write.batch.size=25
# This property determines the number of objects to keep in Samza's cache, per task instance. This same cache is also used for write buffering (see stores.*.write.batch.size). A value of 0 disables all caching and batching.
stores.hdfs-write-sshscores_top.object.cache.size=100
# The size of LevelDB's block cache in bytes, per container. Note that this is an off-heap memory allocation, so the container's total memory use is the maximum JVM heap size plus the size of this cache.
stores.hdfs-write-sshscores_top.container.cache.size.bytes=2000
# The amount of memory (in bytes) that LevelDB uses for buffering writes before they are written to disk,
stores.hdfs-write-sshscores_top.container.write.buffer.size.bytes=1000

# VPN enriched
stores.hdfs-write-vpnenriched.factory=org.apache.samza.storage.kv.KeyValueStorageEngineFactory
stores.hdfs-write-vpnenriched.changelog=kafka.hdfs-write-vpnenriched-changelog
stores.hdfs-write-vpnenriched.key.serde=string
stores.hdfs-write-vpnenriched.msg.serde=timebarrier
# This property is set to the number of key/value pairs that should be kept in this in-memory buffer, per task instance. The number cannot be greater than stores.*.object.cache.size.
stores.hdfs-write-vpnenriched.write.batch.size=25
# This property determines the number of objects to keep in Samza's cache, per task instance. This same cache is also used for write buffering (see stores.*.write.batch.size). A value of 0 disables all caching and batching.
stores.hdfs-write-vpnenriched.object.cache.size=100
# The size of LevelDB's block cache in bytes, per container. Note that this is an off-heap memory allocation, so the container's total memory use is the maximum JVM heap size plus the size of this cache.
stores.hdfs-write-vpnenriched.container.cache.size.bytes=2000
# The amount of memory (in bytes) that LevelDB uses for buffering writes before they are written to disk.
stores.hdfs-write-vpnenriched.container.write.buffer.size.bytes=1000

# VPN scored
stores.hdfs-write-vpndatares.factory=org.apache.samza.storage.kv.KeyValueStorageEngineFactory
stores.hdfs-write-vpndatares.changelog=kafka.hdfs-write-vpndatares-changelog
stores.hdfs-write-vpndatares.key.serde=string
stores.hdfs-write-vpndatares.msg.serde=timebarrier
# This property is set to the number of key/value pairs that should be kept in this in-memory buffer, per task instance. The number cannot be greater than stores.*.object.cache.size.
stores.hdfs-write-vpndatares.write.batch.size=25
# This property determines the number of objects to keep in Samza's cache, per task instance. This same cache is also used for write buffering (see stores.*.write.batch.size). A value of 0 disables all caching and batching.
stores.hdfs-write-vpndatares.object.cache.size=100
# The size of LevelDB's block cache in bytes, per container. Note that this is an off-heap memory allocation, so the container's total memory use is the maximum JVM heap size plus the size of this cache.
stores.hdfs-write-vpndatares.container.cache.size.bytes=2000
# The amount of memory (in bytes) that LevelDB uses for buffering writes before they are written to disk,
stores.hdfs-write-vpndatares.container.write.buffer.size.bytes=1000

# VPN scored top
stores.hdfs-write-vpndatares_top.factory=org.apache.samza.storage.kv.KeyValueStorageEngineFactory
stores.hdfs-write-vpndatares_top.changelog=kafka.hdfs-write-vpndatares_top-changelog
stores.hdfs-write-vpndatares_top.key.serde=string
stores.hdfs-write-vpndatares_top.msg.serde=timebarrier
# This property is set to the number of key/value pairs that should be kept in this in-memory buffer, per task instance. The number cannot be greater than stores.*.object.cache.size.
stores.hdfs-write-vpndatares_top.write.batch.size=25
# This property determines the number of objects to keep in Samza's cache, per task instance. This same cache is also used for write buffering (see stores.*.write.batch.size). A value of 0 disables all caching and batching.
stores.hdfs-write-vpndatares_top.object.cache.size=100
# The size of LevelDB's block cache in bytes, per container. Note that this is an off-heap memory allocation, so the container's total memory use is the maximum JVM heap size plus the size of this cache.
stores.hdfs-write-vpndatares_top.container.cache.size.bytes=2000
# The amount of memory (in bytes) that LevelDB uses for buffering writes before they are written to disk,
stores.hdfs-write-vpndatares_top.container.write.buffer.size.bytes=1000

# VPN scored session
stores.hdfs-write-vpnsessiondatares.factory=org.apache.samza.storage.kv.KeyValueStorageEngineFactory
stores.hdfs-write-vpnsessiondatares.changelog=kafka.hdfs-write-vpnsessiondatares-changelog
stores.hdfs-write-vpnsessiondatares.key.serde=string
stores.hdfs-write-vpnsessiondatares.msg.serde=timebarrier
# This property is set to the number of key/value pairs that should be kept in this in-memory buffer, per task instance. The number cannot be greater than stores.*.object.cache.size.
stores.hdfs-write-vpnsessiondatares.write.batch.size=25
# This property determines the number of objects to keep in Samza's cache, per task instance. This same cache is also used for write buffering (see stores.*.write.batch.size). A value of 0 disables all caching and batching.
stores.hdfs-write-vpnsessiondatares.object.cache.size=100
# The size of LevelDB's block cache in bytes, per container. Note that this is an off-heap memory allocation, so the container's total memory use is the maximum JVM heap size plus the size of this cache.
stores.hdfs-write-vpnsessiondatares.container.cache.size.bytes=2000
# The amount of memory (in bytes) that LevelDB uses for buffering writes before they are written to disk,
stores.hdfs-write-vpnsessiondatares.container.write.buffer.size.bytes=1000

# top VPN scored session
stores.hdfs-write-vpnsessiondatares_top.factory=org.apache.samza.storage.kv.KeyValueStorageEngineFactory
stores.hdfs-write-vpnsessiondatares_top.changelog=kafka.hdfs-write-vpnsessiondatares_top-changelog
stores.hdfs-write-vpnsessiondatares_top.key.serde=string
stores.hdfs-write-vpnsessiondatares_top.msg.serde=timebarrier
# This property is set to the number of key/value pairs that should be kept in this in-memory buffer, per task instance. The number cannot be greater than stores.*.object.cache.size.
stores.hdfs-write-vpnsessiondatares_top.write.batch.size=25
# This property determines the number of objects to keep in Samza's cache, per task instance. This same cache is also used for write buffering (see stores.*.write.batch.size). A value of 0 disables all caching and batching.
stores.hdfs-write-vpnsessiondatares_top.object.cache.size=100
# The size of LevelDB's block cache in bytes, per container. Note that this is an off-heap memory allocation, so the container's total memory use is the maximum JVM heap size plus the size of this cache.
stores.hdfs-write-vpnsessiondatares_top.container.cache.size.bytes=2000
# The amount of memory (in bytes) that LevelDB uses for buffering writes before they are written to disk,
stores.hdfs-write-vpnsessiondatares_top.container.write.buffer.size.bytes=1000


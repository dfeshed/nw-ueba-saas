# Job
job.factory.class=fortscale.streaming.GracefulShutdownLocalJobFactory
job.name=evidence-creation-task

# Task
task.class=fortscale.streaming.task.EvidenceCreationTask
task.inputs=kafka.fortscale-4769-event-score,kafka.fortscale-ssh-event-score,kafka.fortscale-vpn-event-score,kafka.fortscale-notification-event-score,kafka.fortscale-aggr-feature-events-score,kafka.fortscale-vpnsession-event-score


# do nothing for now - run every 1 hour (60*60*1000=3600000ms)
task.window.ms=3600000

### Fortscale specific task config parameters

# Spring Context
fortscale.context=classpath*:META-INF/spring/streaming-TaggingTask-context.xml

# Output topic: evidences
fortscale.output.topic=fortscale-evidences


# Mapping between input topic and data source configuration
# 4769
fortscale.events.input.topic.4769=fortscale-4769-event-score
fortscale.events.anomalyFields.4769=event_time,source_machine,destination_machine,failure_code
fortscale.events.entityName.field.4769=normalized_username
fortscale.events.entityType.4769=User
fortscale.events.startTimestamp.field.4769=date_time_unix
fortscale.events.endTimestamp.field.4769=date_time_unix
fortscale.events.partition.field.4769=normalized_username
fortscale.events.dataEntitiesIds.4769=kerberos_logins
fortscale.events.evidence.type.4769=AnomalySingleEvent
fortscale.events.score.threshold.4769=75
#fortscale.events.defaultFields.4769=
#fortscale.events.total.field.path.4769=
# SSH
fortscale.events.input.topic.ssh=fortscale-ssh-event-score
fortscale.events.anomalyFields.ssh=event_time,source_machine,destination_machine,auth_method
fortscale.events.entityName.field.ssh=normalized_username
fortscale.events.entityType.ssh=User
fortscale.events.startTimestamp.field.ssh=date_time_unix
fortscale.events.endTimestamp.field.ssh=date_time_unix
fortscale.events.partition.field.ssh=normalized_username
fortscale.events.dataEntitiesIds.ssh=ssh
fortscale.events.evidence.type.ssh=AnomalySingleEvent
fortscale.events.score.threshold.ssh=75
#fortscale.events.defaultFields.ssh=
#fortscale.events.total.field.path.ssh=
# vpn
fortscale.events.input.topic.vpn=fortscale-vpn-event-score
fortscale.events.anomalyFields.vpn=event_time,source_machine,country
fortscale.events.entityName.field.vpn=normalized_username
fortscale.events.entityType.vpn=User
fortscale.events.startTimestamp.field.vpn=date_time_unix
fortscale.events.endTimestamp.field.vpn=date_time_unix
fortscale.events.partition.field.vpn=normalized_username
fortscale.events.dataEntitiesIds.vpn=vpn
fortscale.events.evidence.type.vpn=AnomalySingleEvent
fortscale.events.score.threshold.vpn=75
#fortscale.events.defaultFields.vpn=
#fortscale.events.total.field.path.vpn=
# VPN Session
fortscale.events.input.topic.vpnsession=fortscale-vpnsession-event-score
fortscale.events.anomalyFields.vpnsession=data_bucket
fortscale.events.entityName.field.vpnsession=normalized_username
fortscale.events.entityType.vpnsession=User
fortscale.events.startTimestamp.field.vpnsession=date_time_unix_start
fortscale.events.endTimestamp.field.vpnsession=date_time_unix
fortscale.events.partition.field.vpnsession=normalized_username
fortscale.events.dataEntitiesIds.vpnsession=vpn_session
fortscale.events.evidence.type.vpnsession=AnomalySingleEvent
fortscale.events.score.threshold.vpnsession=0
fortscale.events.preprocess.class.vpnsession=fortscale.streaming.task.evidence.pre.process.VpnSessionEvidencePreProcess
#fortscale.events.postprocess.class.vpnsession=fortscale.streaming.task.evidence.pre.process.VpnSessionPostProcess
# Notification
fortscale.events.input.topic.notification=fortscale-notification-event-score
fortscale.events.scoreField.notification=notification_score
fortscale.events.anomalyValueField.notification=notification_value
fortscale.events.anomalyTypeField.notification=notification_type
fortscale.events.entityName.field.notification=normalized_username
fortscale.events.entityType.notification=User
fortscale.events.startTimestamp.field.notification=date_time_unix_start
fortscale.events.endTimestamp.field.notification=date_time_unix_end
fortscale.events.partition.field.notification=normalized_username
fortscale.events.dataEntitiesIds.field.notification=notification_entity
fortscale.events.evidence.type.notification=Notification
fortscale.events.score.threshold.notification=50
#fortscale.events.total.field.path.notification=
# Aggregated
fortscale.events.input.topic.aggregated=fortscale-aggr-feature-events-score
fortscale.events.scoreField.aggregated=score
fortscale.events.anomalyValueField.aggregated=aggregated_feature_value
fortscale.events.anomalyTypeField.aggregated=aggregated_feature_name
fortscale.events.entityName.field.aggregated=context.normalized_username
fortscale.events.entityType.aggregated=User
fortscale.events.startTimestamp.field.aggregated=start_time_unix
fortscale.events.endTimestamp.field.aggregated=end_time_unix
fortscale.events.partition.field.aggregated=context.normalized_username
fortscale.events.dataEntitiesIds.field.aggregated=data_sources
fortscale.events.evidence.type.aggregated=AnomalyAggregatedEvent
fortscale.events.score.threshold.aggregated=75
fortscale.events.total.field.path.aggregated=aggregated_feature_info.total


# Serializers
serializers.registry.json.class=org.apache.samza.serializers.JsonSerdeFactory
serializers.registry.evidence_object.class=fortscale.streaming.serialization.EvidenceSerdeFactory
serializers.registry.string.class=org.apache.samza.serializers.StringSerdeFactory
serializers.registry.integer.class=org.apache.samza.serializers.IntegerSerdeFactory
serializers.registry.metrics.class=org.apache.samza.serializers.MetricsSnapshotSerdeFactory


# Metric report every 60 seconds to a kafka topic called metrics and as a monitor report
metrics.reporter.snapshot.class=org.apache.samza.metrics.reporter.MetricsSnapshotReporterFactory
metrics.reporter.snapshot.stream=kafka.metrics
systems.kafka.streams.metrics.samza.msg.serde=metrics
metrics.reporters=snapshot


# Systems
systems.kafka.samza.factory=org.apache.samza.system.kafka.KafkaSystemFactory
systems.kafka.samza.msg.serde=string
systems.kafka.samza.offset.default=oldest
systems.kafka.consumer.zookeeper.connect=localhost:2181
systems.kafka.consumer.auto.offset.reset=smallest
systems.kafka.producer.metadata.broker.list=localhost:9092
systems.kafka.producer.producer.type=sync
systems.kafka.producer.retry.backoff.ms = 10000
systems.kafka.producer.acks = 1
systems.kafka.producer.reconnect.backoff.ms = 10000
systems.kafka.producer.key.serializer.class=kafka.serializer.StringEncoder
systems.kafka.producer.queue.buffering.max.ms=2000

# Batch side for writing to output topic
systems.kafka.producer.batch.num.messages=1000

# Declare that we want our job's checkpoints to be enabled and written to Kafka
task.checkpoint.factory=org.apache.samza.checkpoint.kafka.KafkaCheckpointManagerFactory
task.checkpoint.replication.factor=1
task.checkpoint.system=kafka



# Job
job.factory.class=fortscale.streaming.GracefulShutdownLocalJobFactory
job.name=evidence-creation-task

# Task
task.class=fortscale.streaming.task.EvidenceCreationTask
task.inputs=kafka.fortscale-amt-event-score,kafka.fortscale-4769-event-score,kafka.fortscale-ssh-event-score,kafka.fortscale-vpn-event-score


# export the evidences to mongodb every 15 minutes (15*60*1000=900000ms)
task.window.ms=900000

### Fortscale specific task config parameters

# Spring Context
fortscale.context=classpath*:META-INF/spring/streaming-TaggingTask-context.xml

# Output topic: evidences
fortscale.output.topic=fortscale-evidences

# Event time
fortscale.timestamp.field=date_time_unix

# Threshold for creating evidences
fortscale.score.threshold=75

# Mapping between input topic and data source configuration
# 4769
fortscale.events.input.topic.4769=fortscale-4769-event-score
fortscale.events.score.fields.4769=user_score1,user_score2
fortscale.events.normalizedusername.field.4769=normalized_username
fortscale.events.partition.field.4769=normalized_username
fortscale.events.classifier.4769=login
# SSH
fortscale.events.input.topic.ssh=fortscale-ssh-event-score
fortscale.events.score.fields.ssh=user_score1,user_score2
fortscale.events.normalizedusername.field.ssh=normalized_username
fortscale.events.partition.field.ssh=normalized_username
fortscale.events.classifier.ssh=ssh
# vpn
fortscale.events.input.topic.vpn=fortscale-vpn-event-score
fortscale.events.score.fields.vpn=user_score1,user_score2
fortscale.events.normalizedusername.field.vpn=normalized_username
fortscale.events.partition.field.vpn=normalized_username
fortscale.events.classifier.vpn=vpn
#AMT
fortscale.events.input.topic.amt=fortscale-amt-event-score
fortscale.events.score.fields.amt=user_score1,user_score2
fortscale.events.normalizedusername.field.amt=normalized_username
fortscale.events.partition.field.amt=normalized_username
fortscale.events.classifier.amt=amt


# Serializers
serializers.registry.json.class=org.apache.samza.serializers.JsonSerdeFactory
serializers.registry.jsonmodel.class=fortscale.streaming.serialization.UserInfoUpdateSerdFactory
serializers.registry.string.class=org.apache.samza.serializers.StringSerdeFactory
serializers.registry.integer.class=org.apache.samza.serializers.IntegerSerdeFactory
serializers.registry.metrics.class=org.apache.samza.serializers.MetricsSnapshotSerdeFactory


# Metric report every 60 seconds to a kafka topic called metrics and as a monitor report
metrics.reporter.snapshot.class=org.apache.samza.metrics.reporter.MetricsSnapshotReporterFactory
metrics.reporter.snapshot.stream=kafka.metrics
systems.kafka.streams.metrics.samza.msg.serde=metrics
metrics.reporters=snapshot


# Systems
systems.kafka.samza.factory=org.apache.samza.system.kafka.KafkaSystemFactory
systems.kafka.samza.msg.serde=string
systems.kafka.samza.offset.default=oldest
systems.kafka.consumer.zookeeper.connect=localhost:2181
systems.kafka.consumer.auto.offset.reset=smallest
systems.kafka.producer.metadata.broker.list=localhost:9092
systems.kafka.producer.producer.type=sync
systems.kafka.producer.retry.backoff.ms = 10000
systems.kafka.producer.acks = 1
systems.kafka.producer.reconnect.backoff.ms = 10000

# Batch side for writing to output topic
systems.kafka.producer.batch.num.messages=1000

# Declare that we want our job's checkpoints to be enabled and written to Kafka
task.checkpoint.factory=org.apache.samza.checkpoint.kafka.KafkaCheckpointManagerFactory
task.checkpoint.replication.factor=1
task.checkpoint.system=kafka

# Key-value storage evidences (we're coping it to Mongo from time to time)
stores.evidences.factory=org.apache.samza.storage.kv.KeyValueStorageEngineFactory
stores.evidences.key.serde=string
stores.evidences.msg.serde=jsonmodel
stores.evidences.write.batch.size=25
stores.evidences.object.cache.size=100000
stores.evidences.container.cache.size.bytes=419430400
stores.evidences.container.write.buffer.size.bytes=102400

<?xml version='1.0' encoding='utf-8'?>
<job-scheduling-data
        xmlns="http://www.quartz-scheduler.org/xml/JobSchedulingData"
        xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
        xsi:schemaLocation="http://www.quartz-scheduler.org/xml/JobSchedulingData http://www.quartz-scheduler.org/xml/job_scheduling_data_2_0.xsd"
        version="2.0">

    <schedule>
        <job>
            <name>ETL</name>
            <group>DLPMAIL</group>
            <description>process Dlpmail events to etl</description>
            <job-class>fortscale.collection.jobs.event.process.EventProcessJob</job-class> <!-- Important! all generic data sources should have EventProcessJob class! -->
            <durability>true</durability>
            <recover>true</recover>

            <job-data-map>
                <entry>
                    <key>filesFilter</key>
                    <value>DLPMAIL_\d+.csv$</value>
                </entry>
                <entry>
                    <key>outputFields</key>
                    <value>${impala.data.dlpmail.table.fields}</value> <!-- the fields that would be saved to hfds. -->
                </entry>
                <entry>
                    <key>messageOutputFields</key> <!-- the fields that would be passed to streaming. basically, all the outputFields + data_source and last_state fields. -->
                    <value>${kafka.dlpmail.message.record.fields}</value>
                </entry>
                <entry>
                    <key>timestampField</key>
                    <value>${impala.data.dlpmail.table.field.epochtime}</value>
                </entry>
                <entry>
                    <key>outputSeparator</key>
                    <value>${impala.data.dlpmail.table.delimiter}</value>
                </entry>
                <entry>
                    <key>morphlineFile</key>
                    <value>${readDlpmail.morphline}</value>
                </entry>
                <entry>
                    <key>morphlineEnrichment</key>
                    <value>${enrichdlpmail.morphline}</value>
                </entry>
                <entry>
                    <key>hadoopPath</key>
                    <value>${hdfs.user.data.dlpmail.path}</value>
                </entry>
                <entry>
                    <key>hadoopFilename</key> <!-- the file in hdfs that all the ETLed data would be saved -->
                    <value>${impala.data.dlpmail.table.name}.csv</value>
                </entry>
                <entry>
                    <key>impalaTableName</key>
                    <value>${impala.data.dlpmail.table.name}</value>
                </entry>
                <entry>
                    <key>partitionKeyFields</key>
                    <value>${impala.table.fields.username}</value>
                </entry>
                <entry>
                    <key>streamingTopic</key> <!-- no editing needed -->
                    <value>${kafka.generic.data.access.struct.topic}</value>
                </entry>
                <entry>
                    <key>partitionStrategy</key> <!--  monthly, weekly or daily. -->
                    <value>${impala.data.dlpmail.table.partition.type}</value>
                </entry>
            </job-data-map>
        </job>

        <!-- idan perez Remove this trigger
        <trigger>
            <cron>
                <name>dlpmail_send_to_streaming_hourly_trigger</name>
                <group>DLPMAIL</group>
                <job-name>Scoring</job-name>
                <job-group>DLPMAIL</job-group>
                <cron-expression>0 30 * * * ?</cron-expression>
            </cron>
        </trigger>
        -->



    </schedule>

</job-scheduling-data>

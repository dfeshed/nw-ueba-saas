<?xml version='1.0' encoding='utf-8'?>
<job-scheduling-data
        xmlns="http://www.quartz-scheduler.org/xml/JobSchedulingData"
        xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
        xsi:schemaLocation="http://www.quartz-scheduler.org/xml/JobSchedulingData http://www.quartz-scheduler.org/xml/job_scheduling_data_2_0.xsd"
        version="2.0">

    <schedule>
        <job>
            <name>ETL</name>
            <group>DLPFILE</group>
            <description>process Dlp file events to etl</description>
            <job-class>fortscale.collection.jobs.event.process.EventProcessJob</job-class> <!-- Important! all generic data sources should have EventProcessJob class! -->
            <durability>true</durability>
            <recover>true</recover>

            <job-data-map>
                <entry>
                    <key>filesFilter</key>
                    <value>DLPFILE_\d+.csv$</value>
                </entry>
                <entry>
                    <key>outputFields</key>
                    <value>${impala.data.dlpfile.table.fields}</value> <!-- the fields that would be saved to hfds. -->
                </entry>
                <entry>
                    <key>messageOutputFields</key> <!-- the fields that would be passed to streaming. basically, all the outputFields + data_source and last_state fields. -->
                    <value>${kafka.dlpfile.message.record.fields}</value>
                </entry>
                <entry>
                    <key>timestampField</key>
                    <value>${impala.data.dlpfile.table.field.epochtime}</value>
                </entry>
                <entry>
                    <key>outputSeparator</key>
                    <value>${impala.data.dlpfile.table.delimiter}</value>
                </entry>
                <entry>
                    <key>morphlineFile</key>
                    <value>${readDlpfile.morphline}</value>
                </entry>
                <entry>
                    <key>morphlineEnrichment</key>
                    <value>${enrichdlpfile.morphline}</value>
                </entry>
                <entry>
                    <key>hadoopPath</key>
                    <value>${hdfs.user.data.dlpfile.path}</value>
                </entry>
                <entry>
                    <key>hadoopFilename</key> <!-- the file in hdfs that all the ETLed data would be saved -->
                    <value>${impala.data.dlpfile.table.name}.csv</value>
                </entry>
                <entry>
                    <key>impalaTableName</key>
                    <value>${impala.data.dlpfile.table.name}</value>
                </entry>
                <entry>
                    <key>partitionKeyFields</key>
                    <value>${impala.table.fields.username}</value>
                </entry>
                <entry>
                    <key>streamingTopic</key> <!-- no editing needed -->
                    <value>${kafka.generic.data.access.struct.topic}</value>
                </entry>
                <entry>
                    <key>partitionStrategy</key> <!--  monthly, weekly or daily. -->
                    <value>${impala.data.dlpfile.table.partition.type}</value>
                </entry>
            </job-data-map>
        </job>
        <!-- for PS
       <trigger>
           <cron>
               <name>process_dlpfile_events_to_etl_hourly_trigger</name>
               <group>DLPFILE</group>
               <job-name>ETL</job-name>
               <job-group>DLPFILE</job-group>
               <cron-expression>0 30 * * * ?</cron-expression>
           </cron>
       </trigger>
       -->
    </schedule>
</job-scheduling-data>

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Automatic Fs low values reduction calculator\n",
    "This script automatically calculates the config parameter `min_value_for_not_reduce` for Fs small values reduction (for noisy Fs). It was written both for research (prior to writing production code) and for POCs (running versions of the product without automatic Fs low values reduction calculation in the production code).\n",
    "\n",
    "### Assumptions:\n",
    "This script assumes that there's an accessible mongo with the collections starting with `scored___aggr_event` populated. These are used in order to find which Fs are noisy (such that low values reduction can help reducing the noise).\n",
    "\n",
    "### Configuration:\n",
    "* `mongo_ip` should be configured with the right ip.\n",
    "* `verbose` can be set to `True` in order to print more stuff.\n",
    "* `show_graphs` should be set to `True` only when you want to display graphs (typically in research environment).\n",
    "* `aggregated_feature_event_prevalance_stats_path` is the path to the version of the configuration installed for the customer. The reason this is needed is so we can undo the reduction done in runtime - so we can see the real values and scores and decide on the right new reduction (which might be different than what we've set during the installation process).\n",
    "\n",
    "### Output:\n",
    "The names of the Fs that should be reduced are printed following by a number - this is the `min_value_for_not_reduce` parameter. All the other parameters (`max_value_for_fully_reduce` and `reducing_factor`) should be set manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#mongo_ip = 'localhost'\n",
    "mongo_ip = '192.168.45.44'\n",
    "#mongo_ip = 'tc-agent3'\n",
    "verbose = True\n",
    "aggregated_feature_event_prevalance_stats_path = r'C:\\Users\\yoelz\\projects\\fortscale-core\\fortscale\\fortscale-streaming\\config\\aggregated-feature_event-prevalance-stats.properties'\n",
    "#aggregated_feature_event_prevalance_stats_path = '/home/cloudera/fortscale/streaming/config/aggregated-feature_event-prevalance-stats.properties'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    get_ipython()\n",
    "    show_graphs = True\n",
    "    import matplotlib.pyplot as plt\n",
    "    %matplotlib inline\n",
    "except NameError:\n",
    "    show_graphs = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pymongo\n",
    "import itertools\n",
    "import copy\n",
    "import json\n",
    "import re\n",
    "import time\n",
    "import datetime\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "from entities import Entities\n",
    "from utils import print_verbose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#%%javascript\n",
    "#//IPython.load_extensions('usability\\\\execute_time\\\\ExecuteTime');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "start_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "db = pymongo.MongoClient(mongo_ip, 27017).fortscale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def query_hist_of_value_and_score(collection):\n",
    "    res = collection.aggregate([\n",
    "            {\n",
    "                '$project': {\n",
    "                    'aggregated_feature_value': 1,\n",
    "                    'score': 1\n",
    "                }\n",
    "            },\n",
    "            {\n",
    "                '$group': {\n",
    "                    '_id': {\n",
    "                        'aggregated_feature_value': '$aggregated_feature_value',\n",
    "                        'score': '$score'\n",
    "                    },\n",
    "                    'count': {'$sum': 1}\n",
    "                }\n",
    "            },\n",
    "            {\n",
    "                '$project': {\n",
    "                    'aggregated_feature_value': '$_id.aggregated_feature_value',\n",
    "                    'score': '$_id.score',\n",
    "                    'count': 1,\n",
    "                    '_id': 0\n",
    "                }\n",
    "            },\n",
    "            {\n",
    "                '$sort': {\n",
    "                    'score': 1\n",
    "                }\n",
    "            }\n",
    "        ])\n",
    "    if type(res) == dict:\n",
    "        res = res['result'] # in some versions of pymongo the result is a dict (with  'ok' and 'result' fields) instead of a cursor\n",
    "    else:\n",
    "        res = list(res)\n",
    "    return dict(((entry['aggregated_feature_value'], entry['score']), entry['count']) for entry in res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_all_collection_names():\n",
    "    if pymongo.version_tuple[0] > 2 or (pymongo.version_tuple[0] == 2 and pymongo.version_tuple[1] > 7):\n",
    "        return db.collection_names()\n",
    "    else:\n",
    "        return [e['name'] for e in db.command('listCollections')['cursor']['firstBatch'] if e['name'].startswith('scored___aggr_event')]\n",
    "\n",
    "def load_fs_from_mongo():\n",
    "    f_collection_names = filter(lambda name : name.startswith('scored___aggr_event'), get_all_collection_names())\n",
    "    print_verbose('collection names:')\n",
    "    for collection_name in f_collection_names:\n",
    "        print_verbose('\\t', collection_name)\n",
    "    print\n",
    "    fs = {}\n",
    "    for f_type in ['hourly', 'daily']:\n",
    "        fs[f_type] = {}\n",
    "        for collection_name in f_collection_names:\n",
    "            if collection_name.endswith(f_type):\n",
    "                print_verbose('quering', collection_name, '...')\n",
    "                fs[f_type][collection_name[:-len('_' + f_type)]] = query_hist_of_value_and_score(db[collection_name])\n",
    "    return fs\n",
    "\n",
    "def load_old_low_values_reducers():\n",
    "    '''\n",
    "    TODO: this function was copied from \"alphas and betas.ipynb\". it should be refactored into a utils library.\n",
    "    '''\n",
    "    res = {}\n",
    "    with open(aggregated_feature_event_prevalance_stats_path, 'r') as f:\n",
    "        for l in f.readlines():\n",
    "            match = re.search('fortscale\\.aggr_event\\..*\\.(.*)\\.fortscale\\.score\\..*\\.reduction\\.configs=.*\"reducingFactor\":(\\d+\\.?\\d*).*\"maxValueForFullyReduce\":(\\d+\\.?\\d*).*\"minValueForNotReduce\":(\\d+\\.?\\d*)', l)\n",
    "            if match is not None:\n",
    "                f_name, reducing_factor, max_value_for_fully_reduce, min_value_for_not_reduce = match.groups()\n",
    "                if res.has_key(f_name):\n",
    "                    raise Exception(f_name + ' was already encountered')\n",
    "                res[f_name] = {\n",
    "                    'reducing_factor': float(reducing_factor),\n",
    "                    'max_value_for_fully_reduce': float(max_value_for_fully_reduce),\n",
    "                    'min_value_for_not_reduce': float(min_value_for_not_reduce)\n",
    "                }\n",
    "    return res\n",
    "\n",
    "def calc_reducing_factor(value, min_value_for_not_reduce, max_value_for_fully_reduce, reducing_factor):\n",
    "    '''\n",
    "    TODO: this function was copied from \"alphas and betas.ipynb\". it should be refactored into a utils library.\n",
    "    '''\n",
    "    if value <= max_value_for_fully_reduce:\n",
    "        factor = reducing_factor\n",
    "    elif value < min_value_for_not_reduce:\n",
    "        numerator = value - max_value_for_fully_reduce\n",
    "        denominator = min_value_for_not_reduce - max_value_for_fully_reduce\n",
    "        part_to_add = 1. * numerator / denominator\n",
    "        factor = reducing_factor + (1 - reducing_factor) * part_to_add\n",
    "    else:\n",
    "        factor = 1\n",
    "    return factor\n",
    "\n",
    "def cancel_low_values_reduction_for_single_score(score, value, min_value_for_not_reduce, max_value_for_fully_reduce, reducing_factor):\n",
    "    if min_value_for_not_reduce != None:\n",
    "        score /= calc_reducing_factor(value, min_value_for_not_reduce, max_value_for_fully_reduce, reducing_factor)\n",
    "    return score\n",
    "\n",
    "def cancel_low_values_reduction(fs):\n",
    "    old = load_old_low_values_reducers()\n",
    "    fs = copy.deepcopy(fs)\n",
    "    for f_type in ['hourly', 'daily']:\n",
    "        for collection_name, hist in list(fs[f_type].iteritems()):\n",
    "            f_name = collection_name + '_' + f_type\n",
    "            f_name = f_name[len('scored___aggr_event__'):]\n",
    "            old_f_reducer = old.get(f_name, None)\n",
    "            if old_f_reducer is None:\n",
    "                continue\n",
    "            unreduced_hist = {}\n",
    "            for value_and_score, count in hist.iteritems():\n",
    "                unreduced_score = cancel_low_values_reduction_for_single_score(value_and_score[1],\n",
    "                                                                               value_and_score[0],\n",
    "                                                                               min_value_for_not_reduce = old_f_reducer['min_value_for_not_reduce'],\n",
    "                                                                               max_value_for_fully_reduce = old_f_reducer['max_value_for_fully_reduce'],\n",
    "                                                                               reducing_factor = old_f_reducer['reducing_factor'])\n",
    "                unreduced_hist[(value_and_score[0], unreduced_score)] = count\n",
    "            fs[f_type][collection_name] = unreduced_hist\n",
    "    return fs\n",
    "\n",
    "try:\n",
    "    fs_before_canceling_reduction\n",
    "except NameError:\n",
    "    fs_before_canceling_reduction = load_fs_from_mongo()\n",
    "fs = cancel_low_values_reduction(fs_before_canceling_reduction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def create_values_hist(hist, score_to_weight):\n",
    "    hist = [{'value': value_and_score[0], 'count': score_to_weight(value_and_score[1]) * count} for value_and_score, count in hist.iteritems()]\n",
    "    hist = [entry for entry in hist if entry['count'] > 0]\n",
    "    hist = sorted(hist, key = lambda entry: entry['value'])\n",
    "    hist = [{'value': value, 'count': sum(entry['count']\n",
    "                                          for entry in entries_with_same_value)}\n",
    "            for value, entries_with_same_value in itertools.groupby(hist, lambda entry: entry['value'])]\n",
    "    return dict((entry['value'], entry['count']) for entry in hist)\n",
    "    \n",
    "def show_hist(hist, score_to_weight, max_value = 20):\n",
    "    hist = create_values_hist(hist, score_to_weight)\n",
    "    hist = dict(filter(lambda value_and_count: value_and_count[0] <= max_value, hist.iteritems()))\n",
    "    print_verbose(hist)\n",
    "    if not show_graphs:\n",
    "        return\n",
    "    # if hist has only one entry, matplotlib will fail to plot it:\n",
    "    hist[0] = hist.get(0, 0.00001)\n",
    "    hist[1] = hist.get(1, 0.00001)\n",
    "    fig, ax = plt.subplots()\n",
    "    fig.set_figwidth(20)\n",
    "    fig.set_figheight(3)\n",
    "    plt.xlim(0, max_value)\n",
    "    plt.hist([val for val in hist],\n",
    "             weights = list(hist.itervalues()),\n",
    "             bins = 1000,\n",
    "             histtype='stepfilled')\n",
    "    plt.xticks(range(max_value))\n",
    "    plt.xlabel('value', fontsize = 20)\n",
    "    plt.ylabel('count', fontsize = 20)\n",
    "    plt.show()\n",
    "    \n",
    "def show_hists(fs, f_type, score_to_weight, max_value = 20):\n",
    "    for collection_name, hist in fs[f_type].iteritems():\n",
    "        collection_name = collection_name + '_' + f_type\n",
    "        print_verbose()\n",
    "        print_verbose(collection_name + ':')\n",
    "        print_verbose('(creating the histogram using score_to_weight)')\n",
    "        show_hist(hist, score_to_weight = score_to_weight, max_value = max_value)\n",
    "        \n",
    "def create_score_to_weight_squared(min_score):\n",
    "    def score_to_weight_squared(score):\n",
    "        return max(0, 1 - ((score - 100) / (100.0 - min_score)) ** 2)\n",
    "    return score_to_weight_squared\n",
    "\n",
    "score_to_weight_squared_min_80 = create_score_to_weight_squared(80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "max_value = 20\n",
    "show_hists(fs, 'daily', score_to_weight = score_to_weight_squared_min_80, max_value = max_value)\n",
    "show_hists(fs, 'hourly', score_to_weight = score_to_weight_squared_min_80, max_value = max_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def calc_min_value_for_not_reduce(hist, score_to_weight, max_value = 20):\n",
    "    hist = create_values_hist(hist, score_to_weight)\n",
    "    total_count = sum(hist.itervalues())\n",
    "    if total_count == 0:\n",
    "        return None\n",
    "    cumsum = 0\n",
    "    prev_count = 0\n",
    "    max_count_seen = 0\n",
    "    peek_start = None\n",
    "    min_value_for_not_reduce = None\n",
    "    for value, count in sorted(hist.iteritems(), key = lambda value_and_count: value_and_count[0]):\n",
    "        # don't overdo it (we don't want to reduce everything)\n",
    "        if 1. * cumsum / total_count > 0.5 and (peek_start is None or value - peek_start > 1):\n",
    "            break\n",
    "        \n",
    "        # don't bother if there are not enough candidates to be considered as noise:\n",
    "        is_enough_noise_absolutely = count > 10\n",
    "        \n",
    "        # peeks can exist only at low values:\n",
    "        if value <= max_value:\n",
    "            if peek_start is None and is_enough_noise_absolutely and 1. * (count - max_count_seen) / total_count > 0.15:\n",
    "                peek_start = value\n",
    "            if peek_start is not None and 1. * count / (prev_count + 1) < 0.85:\n",
    "                min_value_for_not_reduce = value\n",
    "                break\n",
    "        \n",
    "        cumsum += count\n",
    "        prev_count = count\n",
    "        max_count_seen = max(max_count_seen, count)\n",
    "        \n",
    "    return min_value_for_not_reduce\n",
    "\n",
    "def calc_min_value_for_not_reduce_for_hists(fs, f_type, score_to_weight, max_value = 20):\n",
    "    print\n",
    "    print '----------------------------------------------------------------------'\n",
    "    print '-------- Calculating min_value_for_not_reduce for all', f_type, 'Fs -------'\n",
    "    print '----------------------------------------------------------------------'\n",
    "    for collection_name, hist in fs[f_type].iteritems():\n",
    "        collection_name = collection_name + '_' + f_type\n",
    "        min_value_for_not_reduce = calc_min_value_for_not_reduce(hist, score_to_weight = score_to_weight, max_value = max_value)\n",
    "        if min_value_for_not_reduce is not None:\n",
    "            print collection_name + ':', min_value_for_not_reduce\n",
    "    \n",
    "calc_min_value_for_not_reduce_for_hists(fs, 'daily', score_to_weight = score_to_weight_squared_min_80)\n",
    "calc_min_value_for_not_reduce_for_hists(fs, 'hourly', score_to_weight = score_to_weight_squared_min_80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print_verbose(\"The script's run time was\", datetime.timedelta(seconds = int(time.time() - start_time)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "entities = Entities(path = 'entities-TCAGENT7-new.txt', mongo_ip = 'tc-agent7')\n",
    "print 'Querying entities...'\n",
    "if entities.query(start_time = None, end_time = None):\n",
    "    print 'Saving...'\n",
    "    entities.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def find_bad_values(entities, score_to_weight, is_daily):\n",
    "    f_to_bad_values_hist = {}\n",
    "    f_to_contexts = {}\n",
    "    m = (0, None)\n",
    "    for e in sorted(entities.iterate(is_daily = is_daily), key = lambda e: e['startTime']):\n",
    "        for a in e['includedAggrFeatureEvents']:\n",
    "            if a['type'] != 'F':\n",
    "                continue\n",
    "            context_to_history = f_to_contexts.setdefault(a['name'], {})\n",
    "            history = context_to_history.setdefault(e['contextId'], [])\n",
    "            weight = score_to_weight(a['score'])\n",
    "            if len(history) > 0 and abs(sum(history) / len(history) - a['value']) < 2 and weight > 0:\n",
    "                bad_values_hist = f_to_bad_values_hist.setdefault(a['name'], {})\n",
    "                bad_values_hist[a['value']] = bad_values_hist.get(a['value'], 0) + weight\n",
    "            history.append(a['value'])\n",
    "            if len(history) > m[0]:\n",
    "                m = (len(history), a['name'], e['contextId'])\n",
    "    print m\n",
    "    return f_to_bad_values_hist, f_to_contexts\n",
    "    \n",
    "res = find_bad_values(entities, score_to_weight = score_to_weight_squared_min_80, is_daily = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "res[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
